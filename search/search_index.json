{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PRS Japan Workshop has finished (September 1st, 2024). Please help us by clicking here to complete our anonymous post-workshop survey! Workship lecture slides can be downloaded here! For additional questions, don't hesitate to email the speakers: 1. Yuki Okada - Director [yuki-okada at m.u-tokyo.ac.jp] 2. Paul O'Reilly - Day 1&2 [paul.oreilly at mssm.edu] 3. Tade Souaiaia - Day 1 [tade.souaiaia at downstate.edu] 4. Clive Hoggart - Day 2 [clive.hoggart at mssm.edu] Download workshop practical data! Attendees should complete the pre-workshop checklist! Download pre-workshop materials here. Join the workshop google-group here. This short workshop will equip scientists with the tools and approaches required to perform polygenic risk score (PRS) analyses. The workshop will include both applied and theoretical topics in PRS research, delivered across multiple lectures, seminars and computational practicals. In order that students are properly prepared for the workshop, then they must complete this \"Pre-Workshop Guide\" first, by going through each of the sections shown on the left of the screen, starting with 'Checklist'.","title":"Home"},{"location":"#prs-japan-workshop-has-finished-september-1st-2024","text":"Please help us by clicking here to complete our anonymous post-workshop survey! Workship lecture slides can be downloaded here! For additional questions, don't hesitate to email the speakers: 1. Yuki Okada - Director [yuki-okada at m.u-tokyo.ac.jp] 2. Paul O'Reilly - Day 1&2 [paul.oreilly at mssm.edu] 3. Tade Souaiaia - Day 1 [tade.souaiaia at downstate.edu] 4. Clive Hoggart - Day 2 [clive.hoggart at mssm.edu] Download workshop practical data! Attendees should complete the pre-workshop checklist! Download pre-workshop materials here. Join the workshop google-group here. This short workshop will equip scientists with the tools and approaches required to perform polygenic risk score (PRS) analyses. The workshop will include both applied and theoretical topics in PRS research, delivered across multiple lectures, seminars and computational practicals. In order that students are properly prepared for the workshop, then they must complete this \"Pre-Workshop Guide\" first, by going through each of the sections shown on the left of the screen, starting with 'Checklist'.","title":"PRS Japan Workshop has finished (September 1st, 2024)."},{"location":"misc_linux/","text":"Linux for Windows Users For windows users, a single WSL command can be used to install linux. Detailed information on WSL can be found on the microsoft website and step-by-step video obstructions can be found here . These steps involve: Installation Using Administrator Mode to Installing Linux Open PowerShell or Windows Command Prompt by right-clicking and selecting \"Run as administrator\", and type: wsl --install -d Ubuntu This command will enable the features necessary to run WSL, restart your computer, and install the Ubuntu distribution of Linux. After it has been installed you will need to create a new username and password, to learn more about this please see microsoft best practices Setup Once Linux has been installed and you have created a new account you can run Ubuntu using windows terminal or by going to the start menu and typing \"Ubuntu\". Additionally, you should see that Ubuntu has been added to your applications and that there is a Ubuntu Folder on your computer (alongside Desktop, Downloads, etc). Double clicking the Ubuntu app will open up the Ubuntu terminal in the Ubuntu folder . You should see many sub-folders within the Ubuntu folder (such as bin, home, etc, etc.) . You can view them using terminal commands or by double-clicking on them. If you go into the home/ folder you should see another folder with your username. This folder ( home/(your username)/ ) is where you should store your workshop folders and run programs during the workshop. Once you have successfully installed Ubuntu, you can follow the rest of the pre-workshop tutorial using the Linux instructions.","title":"Linux For Windows"},{"location":"misc_linux/#linux-for-windows-users","text":"For windows users, a single WSL command can be used to install linux. Detailed information on WSL can be found on the microsoft website and step-by-step video obstructions can be found here . These steps involve:","title":"Linux for Windows Users"},{"location":"misc_linux/#installation","text":"Using Administrator Mode to Installing Linux Open PowerShell or Windows Command Prompt by right-clicking and selecting \"Run as administrator\", and type: wsl --install -d Ubuntu This command will enable the features necessary to run WSL, restart your computer, and install the Ubuntu distribution of Linux. After it has been installed you will need to create a new username and password, to learn more about this please see microsoft best practices","title":"Installation"},{"location":"misc_linux/#setup","text":"Once Linux has been installed and you have created a new account you can run Ubuntu using windows terminal or by going to the start menu and typing \"Ubuntu\". Additionally, you should see that Ubuntu has been added to your applications and that there is a Ubuntu Folder on your computer (alongside Desktop, Downloads, etc). Double clicking the Ubuntu app will open up the Ubuntu terminal in the Ubuntu folder . You should see many sub-folders within the Ubuntu folder (such as bin, home, etc, etc.) . You can view them using terminal commands or by double-clicking on them. If you go into the home/ folder you should see another folder with your username. This folder ( home/(your username)/ ) is where you should store your workshop folders and run programs during the workshop. Once you have successfully installed Ubuntu, you can follow the rest of the pre-workshop tutorial using the Linux instructions.","title":"Setup"},{"location":"misc_plink_problem/","text":"MacOs Permission Errors If you see this error, Do not click: Move To Trash . Instead click on the top right hand corner of the box, or stop running whatever program you are using and follow the instructions below to give your system permission to run downloaded software. to give your system permission to run downloaded software. Shortcut: Users report that this shortcut can also be used to change program permissions: User finder navigate to the file in question. Right click on the icon and select open . A warning will pop-up. Click open and this program can be run in the future. 1- Change Default Settings By default macOS allows you to open apps from the official Mac App Store only, If you have this set as your default you can change this if you: Open System Preferences. Go to the Security & Privacy tab. Click on the lock and enter your password. Change settings to include identified developers (see below): 2- Allow Exceptions Expanding permissions to include \"identified developers\" is required but not sufficient. To obtain further permission to run Plink or any other unapproved program you can: Open System Preferences. Go to Security & Privacy and select the General tab. If this has happened within the hour, this page will give you an override button to open Open Anyway . Enter you password as above and click this button. You will be asked to once more which will create an exception allowing you to run Plink in the future. BridgePRS specific errors If this problem has occured when running bridgePRS and you have moved plink to the trash you will have to recover it. Additionally the empty files created by a failed attempt to run Plink can cause problems if BridgePRS tries to recover your progress. Restarting a bridgePRS run You can avoid this problem by manually deleting your output directory and starting over, or by using the restart flag: $./bridgePRS pipeline go -o out1 --config_files data/afr.config data/eur.config --phenotype y --restart This will force bridgePRS to restart every subprogram from the beginning.","title":"Mac Security"},{"location":"misc_plink_problem/#macos-permission-errors","text":"If you see this error, Do not click: Move To Trash . Instead click on the top right hand corner of the box, or stop running whatever program you are using and follow the instructions below to give your system permission to run downloaded software. to give your system permission to run downloaded software. Shortcut: Users report that this shortcut can also be used to change program permissions: User finder navigate to the file in question. Right click on the icon and select open . A warning will pop-up. Click open and this program can be run in the future.","title":"MacOs Permission Errors"},{"location":"misc_plink_problem/#1-change-default-settings","text":"By default macOS allows you to open apps from the official Mac App Store only, If you have this set as your default you can change this if you: Open System Preferences. Go to the Security & Privacy tab. Click on the lock and enter your password. Change settings to include identified developers (see below):","title":"1- Change Default Settings"},{"location":"misc_plink_problem/#2-allow-exceptions","text":"Expanding permissions to include \"identified developers\" is required but not sufficient. To obtain further permission to run Plink or any other unapproved program you can: Open System Preferences. Go to Security & Privacy and select the General tab. If this has happened within the hour, this page will give you an override button to open Open Anyway . Enter you password as above and click this button. You will be asked to once more which will create an exception allowing you to run Plink in the future.","title":"2- Allow Exceptions"},{"location":"misc_plink_problem/#bridgeprs-specific-errors","text":"If this problem has occured when running bridgePRS and you have moved plink to the trash you will have to recover it. Additionally the empty files created by a failed attempt to run Plink can cause problems if BridgePRS tries to recover your progress. Restarting a bridgePRS run You can avoid this problem by manually deleting your output directory and starting over, or by using the restart flag: $./bridgePRS pipeline go -o out1 --config_files data/afr.config data/eur.config --phenotype y --restart This will force bridgePRS to restart every subprogram from the beginning.","title":"BridgePRS specific errors"},{"location":"prep_list/","text":"Introduction Here we present a pre-workshop preparation guide to get students sufficiently prepared before the workshop begins. In this guide we help students set up the operating system (Linux or macOS), provide tutorials for Bash, R, and Python for students who are unfamiliar or need a refresher, and help students verify that their system is capable of running the software necessary for the workshop. Reminder: All students must complete pre-workship testing and verification At the end of this guide, students are led through a series of software tests . The tests are required to verify that student laptops are properly set-up before the workshop. Students unable to properly configure laptops should msg the workshop google group (see below.) How To: Join the workshop google-group (group name: prsworkshop) Students without a google account (any email can be used) can create one here: creating a google account Use this account to join the workshop group here or search \"prsworkshop\" the main groups page. Post questions/answers to configuration issues and stay tuned for important announcements. Checklist This checklist will help guide you through the pre-workshop requirements: Operating System : Confirm or install a compatible operating system. Terminal Setup : Setting up the terminal, creating workshop directory. Software Installations : Confirm/Install R, Python3, and Plink as well their as required libraries. Software Tutorials : If unfamiliar with bash, R, or python, please complete the included tutorials. Verification/Testing : Download materials, test that everything is in working order ( Required ).","title":"Checklist"},{"location":"prep_list/#introduction","text":"Here we present a pre-workshop preparation guide to get students sufficiently prepared before the workshop begins. In this guide we help students set up the operating system (Linux or macOS), provide tutorials for Bash, R, and Python for students who are unfamiliar or need a refresher, and help students verify that their system is capable of running the software necessary for the workshop. Reminder: All students must complete pre-workship testing and verification At the end of this guide, students are led through a series of software tests . The tests are required to verify that student laptops are properly set-up before the workshop. Students unable to properly configure laptops should msg the workshop google group (see below.) How To: Join the workshop google-group (group name: prsworkshop) Students without a google account (any email can be used) can create one here: creating a google account Use this account to join the workshop group here or search \"prsworkshop\" the main groups page. Post questions/answers to configuration issues and stay tuned for important announcements.","title":"Introduction"},{"location":"prep_list/#checklist","text":"This checklist will help guide you through the pre-workshop requirements: Operating System : Confirm or install a compatible operating system. Terminal Setup : Setting up the terminal, creating workshop directory. Software Installations : Confirm/Install R, Python3, and Plink as well their as required libraries. Software Tutorials : If unfamiliar with bash, R, or python, please complete the included tutorials. Verification/Testing : Download materials, test that everything is in working order ( Required ).","title":"Checklist"},{"location":"prep_os/","text":"Operating System In this workshop, students will run multiple different statistical genetic software programs. Large-scale genetic data analysis is almost always performed using a server or cloud-based system running a Linux operating system. Therefore, a Linux-based operating system is highly recommended for this workshop, and users running MacOS ( see here ) or Windows ( see here ) are encouraged to install Linux on their laptop if possible. However, the workshop can be completed in the MacOS environments (although there are likely to be more compatibility problems with MacOS). Microsoft Windows users ARE REQUIRED to install Linux and, therefore, we provide detailed instructions to do this. This workshop is designed for either of the following operating systems: Linux: A Debian compatible Linux based operating system (Ubuntu, Mint, etc). macOS: A recent version (11+) of the Desktop OS. Microsoft Windows is not supported, but instructions to install Linux can be found here.","title":"Operating System"},{"location":"prep_os/#operating-system","text":"In this workshop, students will run multiple different statistical genetic software programs. Large-scale genetic data analysis is almost always performed using a server or cloud-based system running a Linux operating system. Therefore, a Linux-based operating system is highly recommended for this workshop, and users running MacOS ( see here ) or Windows ( see here ) are encouraged to install Linux on their laptop if possible. However, the workshop can be completed in the MacOS environments (although there are likely to be more compatibility problems with MacOS). Microsoft Windows users ARE REQUIRED to install Linux and, therefore, we provide detailed instructions to do this. This workshop is designed for either of the following operating systems: Linux: A Debian compatible Linux based operating system (Ubuntu, Mint, etc). macOS: A recent version (11+) of the Desktop OS. Microsoft Windows is not supported, but instructions to install Linux can be found here.","title":"Operating System"},{"location":"prep_software/","text":"Required System Software This workshop requires that the following software be installed and tested before the workshop: Name/Link Description Additional Requirements 1. R Most popular analysis program in statistical genetics Yes (Specific Libraries). 2. Python 3 . Multi purpose Programming Language Yes (The matplotlib library). Both popular computer languages, R and python3 will be used during the workshop. Here we will provide a guide for installation of both languages and their associated libraries. 1. R (and associated packages) R is the most popular analysis program in statistical genetics, and required for most genetic analysis. We recommend that MacOS users download an up-to-date (4.4.1) version directly from the R website and using the R installer. If you are having trouble we recommend this video tutorial . While Linux users can also find a suitable version on the website, we recommend using the package installer that comes with Debian based distributions (Debian, Ubuntu, Mint, etc) and downloading R directly using the following terminal commands: sudo apt install r-base # to install R sudo apt install build-essential # to install the essential packages Important: Additional R Packages Are Required This workshop requires the following non-standard R packages: BEDMatrix, boot, data.table, doMC, glmnet, MASS, optparse, parallel, and R.utils These packages can be installed by opening up an R session from within the terminal, by typing: R And typing the following command: install.packages(c(\"BEDMatrix\",\"boot\",\"data.table\",\"doMC\",\"glmnet\",\"MASS\",\"optparse\",\"parallel\",\"R.utils\")) After you have finished installation and testing of R, please consider completing our R-tutorial to better familiarize yourself with some basic analysis commands. 2. Python3 (and matplotlib) Python3 is a popular multiuse programming language. This workshop requires Python3 and the matplotlib library. Any version of Python3 is acceptable but a newer version Python3.10.10+ is recommended. Many systems come installed with Python3 by default, but it can be downloaded from the python website . The matplotlib package can be found here . Users of macOs can find detailed instructions on how to install python3 by visiting the following video link that we find very helpful. After installing python3, macOs users can install matplotlib using pip: python3 -m ensurepip sudo python3 -m pip install -U matplotlib For Linux we again recommend that you use the package installer to install python3 and matplotlib sudo apt install python3 # to install python3 python3 --version # to verify that it worked sudo apt-get install python3-matplotlib # to install matplotlib After you have finished with this section, don't forget to complete our Python tutorial to familiarize yourself with some basic commands.","title":"Software Installs"},{"location":"prep_software/#required-system-software","text":"This workshop requires that the following software be installed and tested before the workshop: Name/Link Description Additional Requirements 1. R Most popular analysis program in statistical genetics Yes (Specific Libraries). 2. Python 3 . Multi purpose Programming Language Yes (The matplotlib library). Both popular computer languages, R and python3 will be used during the workshop. Here we will provide a guide for installation of both languages and their associated libraries.","title":"Required System Software"},{"location":"prep_software/#1-r-and-associated-packages","text":"R is the most popular analysis program in statistical genetics, and required for most genetic analysis. We recommend that MacOS users download an up-to-date (4.4.1) version directly from the R website and using the R installer. If you are having trouble we recommend this video tutorial . While Linux users can also find a suitable version on the website, we recommend using the package installer that comes with Debian based distributions (Debian, Ubuntu, Mint, etc) and downloading R directly using the following terminal commands: sudo apt install r-base # to install R sudo apt install build-essential # to install the essential packages Important: Additional R Packages Are Required This workshop requires the following non-standard R packages: BEDMatrix, boot, data.table, doMC, glmnet, MASS, optparse, parallel, and R.utils These packages can be installed by opening up an R session from within the terminal, by typing: R And typing the following command: install.packages(c(\"BEDMatrix\",\"boot\",\"data.table\",\"doMC\",\"glmnet\",\"MASS\",\"optparse\",\"parallel\",\"R.utils\")) After you have finished installation and testing of R, please consider completing our R-tutorial to better familiarize yourself with some basic analysis commands.","title":"1. R (and associated packages)"},{"location":"prep_software/#2-python3-and-matplotlib","text":"Python3 is a popular multiuse programming language. This workshop requires Python3 and the matplotlib library. Any version of Python3 is acceptable but a newer version Python3.10.10+ is recommended. Many systems come installed with Python3 by default, but it can be downloaded from the python website . The matplotlib package can be found here . Users of macOs can find detailed instructions on how to install python3 by visiting the following video link that we find very helpful. After installing python3, macOs users can install matplotlib using pip: python3 -m ensurepip sudo python3 -m pip install -U matplotlib For Linux we again recommend that you use the package installer to install python3 and matplotlib sudo apt install python3 # to install python3 python3 --version # to verify that it worked sudo apt-get install python3-matplotlib # to install matplotlib After you have finished with this section, don't forget to complete our Python tutorial to familiarize yourself with some basic commands.","title":"2. Python3 (and matplotlib)"},{"location":"prep_terminal/","text":"Setting up the Terminal Terminal programs allow you to navigate the Unix-Based OS, a simple system made up of files and folders. For this workshop, basic familiarity is required. Here we will go over setting up your terminal. If you are unfamiliar with the terminal afterwards it is recommended you complete our bash tutorial . To begin open up a terminal window, by: macOS: Searching for \"terminal\" on top toolbar, or going to Applications/Utilities/ and clicking the icon. Linux: Opening the lower left hand start menu, typing \"terminal\" and clicking on the icon. Note: Users running Ubuntu in Windows can follow the Linux directions after double clicking the Ubuntu App icon to bring up the terminal A window will open up that looks something like this: Bash Type: cd ~ The cd command (learn more) stands for change directory, and ~ represents a shortcut to your \"home\" directory. After typing this command, you are \"in\" your home directory. Now type: pwd The pwd command returns your current \"path\", which represents the name and location of your home directory. Depending on your machine typing pwd from your home directory may return: /home/(your username) (most Linux environments) /Users/(your username) (older macOs, shared systems) Something else. Because the name, location, and syntax differs from computer to computer, we use $HOME as a universal shortcut to represent your home directory. The command ls (learn_more) lists the files and folders in your home directory. These files and folders in the home directory will differ from computer to computer, but most filesystems will have a Desktop and Downloads folder in the home directory: Often the home directory of the filesystem can be quite crowded, which is why we would like to carry out this tutorial in a new directory specifically for the workshop. To create this directory, type: mkdir prsworkshop The mkdir command makes a new directory called prsworkshop that is located in your home directory: Next we can move to this newly created directory by typing: cd prsworkshop Now we will create a file using the program nano by typing: nano test.sh This command will open up the nano text editor: On the top line type: echo \"hello world\" Then save the file using Ctrl-O and press enter, and quit using Ctrl-X and pressing enter. Now type: ls Do you see the file you just created? To see the contents of the file you just created you can type: cat test.sh Do you know that the file you just created can also run as program? To execute the bash code that you have created you can type: bash test.sh Congratulations, you have now navigated the filesystem, created a directory, created a file and executed a program! You may feel like somewhat of an expert. However, if you still feel unfamiliar with shell scripting, please consider completing our longer bash tutorial . Otherwise, proceed to the next step, where you will install all system wide software necessary for the workshop.","title":"Terminal Setup"},{"location":"prep_terminal/#setting-up-the-terminal","text":"Terminal programs allow you to navigate the Unix-Based OS, a simple system made up of files and folders. For this workshop, basic familiarity is required. Here we will go over setting up your terminal. If you are unfamiliar with the terminal afterwards it is recommended you complete our bash tutorial . To begin open up a terminal window, by: macOS: Searching for \"terminal\" on top toolbar, or going to Applications/Utilities/ and clicking the icon. Linux: Opening the lower left hand start menu, typing \"terminal\" and clicking on the icon. Note: Users running Ubuntu in Windows can follow the Linux directions after double clicking the Ubuntu App icon to bring up the terminal A window will open up that looks something like this:","title":"Setting up the Terminal"},{"location":"prep_terminal/#bash","text":"Type: cd ~ The cd command (learn more) stands for change directory, and ~ represents a shortcut to your \"home\" directory. After typing this command, you are \"in\" your home directory. Now type: pwd The pwd command returns your current \"path\", which represents the name and location of your home directory. Depending on your machine typing pwd from your home directory may return: /home/(your username) (most Linux environments) /Users/(your username) (older macOs, shared systems) Something else. Because the name, location, and syntax differs from computer to computer, we use $HOME as a universal shortcut to represent your home directory. The command ls (learn_more) lists the files and folders in your home directory. These files and folders in the home directory will differ from computer to computer, but most filesystems will have a Desktop and Downloads folder in the home directory: Often the home directory of the filesystem can be quite crowded, which is why we would like to carry out this tutorial in a new directory specifically for the workshop. To create this directory, type: mkdir prsworkshop The mkdir command makes a new directory called prsworkshop that is located in your home directory: Next we can move to this newly created directory by typing: cd prsworkshop Now we will create a file using the program nano by typing: nano test.sh This command will open up the nano text editor: On the top line type: echo \"hello world\" Then save the file using Ctrl-O and press enter, and quit using Ctrl-X and pressing enter. Now type: ls Do you see the file you just created? To see the contents of the file you just created you can type: cat test.sh Do you know that the file you just created can also run as program? To execute the bash code that you have created you can type: bash test.sh Congratulations, you have now navigated the filesystem, created a directory, created a file and executed a program! You may feel like somewhat of an expert. However, if you still feel unfamiliar with shell scripting, please consider completing our longer bash tutorial . Otherwise, proceed to the next step, where you will install all system wide software necessary for the workshop.","title":"Bash"},{"location":"prep_testing/","text":"Preworkshop Testing Downloading materials For Linux: Link. For macOs: Link. By clicking the link, then the download ( down-arrow ) icon at the top of the screen. If a message comes up that the file can't be scanned for viruses, please click \"download anyway\". You should already have a folder in your home directory named prsworkshop that you created during the terminal part of this guide by typing \"mkdir ~/prsworkshop\". If you haven't created this folder, please do so now. Next, move to this directory by typing: cd ~/prsworkshop Next unzip the downloaded workshop materials and move them to this directory. This can be down by right clicking on the folder to unzip and then dragging it into the prsworkshop folder, or it can be done in the terminal. In Linux this is accomplished by typing: tar -xvzf ~/Downloads/preworkshop_materials_linux.tar.gz -C ~/prsworkshop cd ~/prsworkshop/preworkshop_materials_linux MacOs safari sometimes unzips downloaded files for you, so macOs users should first type the following: ls ~/Downloads/preworkshop_materials_mac.* If the command returns a file with a \".tar\" ending, then type: tar -xvf ~/Downloads/preworkshop_materials_mac.tar -C ~/prsworkshop Otherwise, if the command returns a file with a \".tar.gz\" ending, type: tar -xvzf ~/Downloads/preworkshop_materials_mac.tar.gz -C ~/prsworkshop To unzip the folder and move it to the appropriate directory. Finally, move to the appropriate directory to begin testing: cd ~/prsworkshop/preworkshop_materials_mac Testing Software Warning For macOs Users: MacOs often block executables if they are not approved from the app store. If you see an error similar to this, when trying to run plink, PRSice, or bridgePRS, DO NOT DELETE THE FILE . You must change your system settings to allow downloaded software. To learn how to do so, please click here . Plink From the preworkshop directory, navigate into the Plink directory. cd Plink and type the following command: ./code/plink -h If no error is observed and a list of options are displayed then your computer is ready to run plink. If you are not already familiar with plink, now is a great time to use the data found in Plink/tutorials/sample_data/ to run the PLINK tutorial . Testing PRSice Please navigate to the correct directory: cd ~/prsworkshop/preworkshop_materials_(mac/linux)/PRSice and type the following command: ./code/PRSice -h # macOs users If no error is observed and a list of options are displayed then your computer is ready to run PRSice. Testing bridgePRS To verify that bridgePRS is able to run navigate to the folder: cd ~/prsworkshop/preworkshop_materials_(mac/linux)/BridgePRS and type the command: ./bridgePRS You should see bridge art. If this command works, then type the following command: ./bridgePRS tools check-requirements To confirm that your system is up to date, all libraries are installed and you are ready to run bridgePRS.","title":"Testing"},{"location":"prep_testing/#preworkshop-testing","text":"","title":"Preworkshop Testing"},{"location":"prep_testing/#downloading-materials","text":"For Linux: Link. For macOs: Link. By clicking the link, then the download ( down-arrow ) icon at the top of the screen. If a message comes up that the file can't be scanned for viruses, please click \"download anyway\". You should already have a folder in your home directory named prsworkshop that you created during the terminal part of this guide by typing \"mkdir ~/prsworkshop\". If you haven't created this folder, please do so now. Next, move to this directory by typing: cd ~/prsworkshop Next unzip the downloaded workshop materials and move them to this directory. This can be down by right clicking on the folder to unzip and then dragging it into the prsworkshop folder, or it can be done in the terminal. In Linux this is accomplished by typing: tar -xvzf ~/Downloads/preworkshop_materials_linux.tar.gz -C ~/prsworkshop cd ~/prsworkshop/preworkshop_materials_linux MacOs safari sometimes unzips downloaded files for you, so macOs users should first type the following: ls ~/Downloads/preworkshop_materials_mac.* If the command returns a file with a \".tar\" ending, then type: tar -xvf ~/Downloads/preworkshop_materials_mac.tar -C ~/prsworkshop Otherwise, if the command returns a file with a \".tar.gz\" ending, type: tar -xvzf ~/Downloads/preworkshop_materials_mac.tar.gz -C ~/prsworkshop To unzip the folder and move it to the appropriate directory. Finally, move to the appropriate directory to begin testing: cd ~/prsworkshop/preworkshop_materials_mac","title":"Downloading materials"},{"location":"prep_testing/#testing-software","text":"Warning For macOs Users: MacOs often block executables if they are not approved from the app store. If you see an error similar to this, when trying to run plink, PRSice, or bridgePRS, DO NOT DELETE THE FILE . You must change your system settings to allow downloaded software. To learn how to do so, please click here .","title":"Testing Software"},{"location":"prep_testing/#plink","text":"From the preworkshop directory, navigate into the Plink directory. cd Plink and type the following command: ./code/plink -h If no error is observed and a list of options are displayed then your computer is ready to run plink. If you are not already familiar with plink, now is a great time to use the data found in Plink/tutorials/sample_data/ to run the PLINK tutorial .","title":"Plink"},{"location":"prep_testing/#testing-prsice","text":"Please navigate to the correct directory: cd ~/prsworkshop/preworkshop_materials_(mac/linux)/PRSice and type the following command: ./code/PRSice -h # macOs users If no error is observed and a list of options are displayed then your computer is ready to run PRSice.","title":"Testing PRSice"},{"location":"prep_testing/#testing-bridgeprs","text":"To verify that bridgePRS is able to run navigate to the folder: cd ~/prsworkshop/preworkshop_materials_(mac/linux)/BridgePRS and type the command: ./bridgePRS You should see bridge art. If this command works, then type the following command: ./bridgePRS tools check-requirements To confirm that your system is up to date, all libraries are installed and you are ready to run bridgePRS.","title":"Testing bridgePRS"},{"location":"tut_R/","text":"Introduction to R R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop. Basics To being type R in the terminal: R Libraries Most functionality of R is organised in packages or libraries. To access these functions, we will have to install and load these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2) Variables in R You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\") Functions You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2) Plotting While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point() Regression Models In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coefficient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial))","title":"R Tutorial"},{"location":"tut_R/#introduction-to-r","text":"R is a useful programming language that allows us to perform a variety of statis- tical tests and data manipulation. It can also be used to generate fantastic data visualisations. Here we will go through some of the basics of R so that you can better understand the practicals throughout the workshop.","title":"Introduction to R"},{"location":"tut_R/#basics","text":"To being type R in the terminal: R","title":"Basics"},{"location":"tut_R/#libraries","text":"Most functionality of R is organised in packages or libraries. To access these functions, we will have to install and load these packages. Most commonly used packages are installed together with the standard installation process. You can install a new library using the install.packages function. For example, to install ggplot2 , run the command: install.packages(\"ggplot2\") After installation, you can load the library by typing library(ggplot2)","title":"Libraries"},{"location":"tut_R/#variables-in-r","text":"You can assign a value or values to any variable you want using \\<-. e.g Assign a number to a a <- 1 Assign a vector containing a,b,c to v1 v1 <- c(\"a\", \"b\",\"c\")","title":"Variables in R"},{"location":"tut_R/#functions","text":"You can perform lots of operations in R using di\ufb00erent built-in R functions. Some examples are below: Assign number of samples nsample <- 10000 Generate nsample random normal variable with mean = 0 and sd = 1 normal <- rnorm(nsample, mean=0,sd=1) normal.2 <- rnorm(nsample, mean=0,sd=1) We can examine the first few entries of the result using head head(normal) And we can obtain the mean and sd using mean(normal) sd(normal) We can also calculate the correlation between two variables using cor cor(normal, normal.2)","title":"Functions"},{"location":"tut_R/#plotting","text":"While R contains many powerful plotting functions in its base packages,customisationn can be di\ufb03cult (e.g. changing the colour scales, arranging the axes). ggplot2 is a powerful visualization package that provides extensive flexibility and customi- sation of plots. As an example, we can do the following Load the package library(ggplot2) Specify sample size nsample <-1000 Generate random grouping using sample with replacement groups <- sample(c(\"a\",\"b\"), nsample, replace=T) Now generate the data dat <- data.frame(x=rnorm(nsample), y=rnorm(nsample), groups) Generate a scatter plot with di\ufb00erent coloring based on group ggplot(dat, aes(x=x,y=y,color=groups))+geom_point()","title":"Plotting"},{"location":"tut_R/#regression-models","text":"In statistical modelling, regression analyses are a set of statistical techniques for estimating the relationships among variables or features. We can perform regression analysis in R . Use the following code to perform linear regression on simulated variables \"x\" and \"y\": Simulate data nsample <- 10000 x <- rnorm(nsample) y <- rnorm(nsample) Run linear regression lm(y~x) We can store the result into a variable reg <- lm(y~x) And get a detailed output using summary summary(lm(y~x)) We can also extract the coe\ufb03cient of regression using reg$coefficient And we can obtain the residuals by residual <- resid(reg) Examine the first few entries of residuals head(residual) We can also include covariates into the model covar <- rnorm(nsample) lm(y~x+covar) And can even perform interaction analysis lm(y~x+covar+x*covar) Alternatively, we can use the glm function to perform the regression: glm(y~x) For binary traits (case controls studies), logistic regression can be performed using Simulate samples nsample <- 10000 x <- rnorm(nsample) Simulate binary traits (must be coded with 0 and 1) y <- sample(c(0,1), size=nsample, replace=T) Perform logistic regression glm(y~x, family=binomial) Obtain the detailed output summary(glm(y~x, family=binomial))","title":"Regression Models"},{"location":"tut_bash/","text":"Bash/Shell Tutorial The ability to navigate the filesystem using bash is a very important skill in statistical genetics. Bash is a programming language commonly used to navigate the terminal and manipulate files and folders. Some terminals run \"bash-like\" scripting languages like \"zsh\". For the purposes of this tutorial, they are indistinguishable from bash. There are approximately 50 bash commands that are used 95% of the time . Here we will review some common commands and do some simple file analysis. Common Commands cd This command is used to change our directory, in the following manner: cd $PATH where $PATH represents the path to the target directory. Common usage of cd includes: cd ~/ # will bring you to your home directory cd $HOME # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd Downloads # will bring you to the Downloads directory, provided that you are in the home directory cd ~/Downloads # will bring you to Downloads, no matter where you are in the filesystem ls This command allows you to look at the contents of a directory: ls Some common usage of ls includes: ls # list the contents of the current directory ls ~ # list the contents of the home directory ls ~/Desktop # Will list the contents of the Desktop For ls , there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size mkdir This command to create a new directory in your home folder: mkdir ~/test_directory The following commands should be carried out within this directory Use cd to enter the directory: cd ~/test_directory wsl echo echo prints to screen: echo \"Hello\" touch touch creates a new (empty) file: touch foo rm rm deletes a file: rm foo ^ The carrot sign ^ sends the output to a file: echo \"Hello\" > output.txt cat cat prints the entire contents of a file to the screen: cat output.txt less less can be used to view a file: less output.txt to return to the terminal press q cp cp copies a file: cat output.txt output2.txt nano nano can be used to edit a file: nano data1.txt Will bring up an editor window: Starting from the top line type: bob 1 fred 2 mary 3 noah 4 sally 5 And then save the file using Ctrl-O and press enter, and quit using Ctrl-X . wc The word count command wc can be used to count the number of lines or words in a file: wc -l data1.txt grep Can be used to search a file for a string: grep \"noah\" data1.txt will return all the lines in data1.txt containing the string \"noah\". File Analysis A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by performing operations on its columns - this is extremely useful for scientific data sets because typically the columns are features or variables of interest. For example, we can use awk to produce a new file that squares the data in our previous file: awk '{print $1,$2*$2}' data1.txt > data2.txt We can also use awk to add up all the squared data values: awk 'BEGIN{total=0} {total+=$2} END{print total}' data2.txt","title":"Bash Tutorial"},{"location":"tut_bash/#bashshell-tutorial","text":"The ability to navigate the filesystem using bash is a very important skill in statistical genetics. Bash is a programming language commonly used to navigate the terminal and manipulate files and folders. Some terminals run \"bash-like\" scripting languages like \"zsh\". For the purposes of this tutorial, they are indistinguishable from bash. There are approximately 50 bash commands that are used 95% of the time . Here we will review some common commands and do some simple file analysis.","title":"Bash/Shell Tutorial"},{"location":"tut_bash/#common-commands","text":"","title":"Common Commands"},{"location":"tut_bash/#cd","text":"This command is used to change our directory, in the following manner: cd $PATH where $PATH represents the path to the target directory. Common usage of cd includes: cd ~/ # will bring you to your home directory cd $HOME # will bring you to your home directory cd ../ # will bring you to the parent directory (up one level) cd Downloads # will bring you to the Downloads directory, provided that you are in the home directory cd ~/Downloads # will bring you to Downloads, no matter where you are in the filesystem","title":"cd"},{"location":"tut_bash/#ls","text":"This command allows you to look at the contents of a directory: ls Some common usage of ls includes: ls # list the contents of the current directory ls ~ # list the contents of the home directory ls ~/Desktop # Will list the contents of the Desktop For ls , there are a number of additional Unix command options that you can append to it to get additional information, for example: ls -l # shows files as list ls -lh # shows files as a list with human readable format ls -lt # shows the files as a list sorted by time-last-edited ls -lS # shows the files as a list sorted by size","title":"ls"},{"location":"tut_bash/#mkdir","text":"This command to create a new directory in your home folder: mkdir ~/test_directory The following commands should be carried out within this directory Use cd to enter the directory: cd ~/test_directory wsl","title":"mkdir"},{"location":"tut_bash/#echo","text":"echo prints to screen: echo \"Hello\"","title":"echo"},{"location":"tut_bash/#touch","text":"touch creates a new (empty) file: touch foo","title":"touch"},{"location":"tut_bash/#rm","text":"rm deletes a file: rm foo","title":"rm"},{"location":"tut_bash/#_1","text":"The carrot sign ^ sends the output to a file: echo \"Hello\" > output.txt","title":"^"},{"location":"tut_bash/#cat","text":"cat prints the entire contents of a file to the screen: cat output.txt","title":"cat"},{"location":"tut_bash/#less","text":"less can be used to view a file: less output.txt to return to the terminal press q","title":"less"},{"location":"tut_bash/#cp","text":"cp copies a file: cat output.txt output2.txt","title":"cp"},{"location":"tut_bash/#nano","text":"nano can be used to edit a file: nano data1.txt Will bring up an editor window: Starting from the top line type: bob 1 fred 2 mary 3 noah 4 sally 5 And then save the file using Ctrl-O and press enter, and quit using Ctrl-X .","title":"nano"},{"location":"tut_bash/#wc","text":"The word count command wc can be used to count the number of lines or words in a file: wc -l data1.txt","title":"wc"},{"location":"tut_bash/#grep","text":"Can be used to search a file for a string: grep \"noah\" data1.txt will return all the lines in data1.txt containing the string \"noah\".","title":"grep"},{"location":"tut_bash/#file-analysis","text":"A very powerful feature of the terminal is the awk programming language, which allows us to extract subsets of a data file, filter data according to some criteria or perform arithmetic operations on the data. awk manipulates a data file by performing operations on its columns - this is extremely useful for scientific data sets because typically the columns are features or variables of interest. For example, we can use awk to produce a new file that squares the data in our previous file: awk '{print $1,$2*$2}' data1.txt > data2.txt We can also use awk to add up all the squared data values: awk 'BEGIN{total=0} {total+=$2} END{print total}' data2.txt","title":"File Analysis"},{"location":"tut_intro/","text":"Introduction Here we present pre-workshop tutorials for bash/shell , R , and Python3 , and Plink . Students who are not already very familiar with these programs are required to complete these tutorials. The first three tutorials do not require any data and can be completed at anytime while the last tutorial (Plink) requires sample data downloaded in Testing . When the workshop begins students will be expected to understand the following: Bash : Navigate filesystem, copy/move/unzip files. Create directories and edit files (using nano). R : Install packages, read from file, manipulate variables, understand functions, create plots. Python : Import libraries, create variables, understand list comprehension, basic plotting. Plink : Explore and genderate GWAS data, recode and reorder allelic data, use PLINK website.","title":"Introduction"},{"location":"tut_intro/#introduction","text":"Here we present pre-workshop tutorials for bash/shell , R , and Python3 , and Plink . Students who are not already very familiar with these programs are required to complete these tutorials. The first three tutorials do not require any data and can be completed at anytime while the last tutorial (Plink) requires sample data downloaded in Testing . When the workshop begins students will be expected to understand the following: Bash : Navigate filesystem, copy/move/unzip files. Create directories and edit files (using nano). R : Install packages, read from file, manipulate variables, understand functions, create plots. Python : Import libraries, create variables, understand list comprehension, basic plotting. Plink : Explore and genderate GWAS data, recode and reorder allelic data, use PLINK website.","title":"Introduction"},{"location":"tut_plink/","text":"Introduction to PLINK (Part I) PLINK is the most popular software program for performing genome-wide association analyses, it is extremely extensive allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials. Sample Data This tutorial runs on the data in the pre-workshop materials downloaded here . If you have followed the previous directions, this data should be in: ~/prsworkshop/preworkshop_materials_(\"your OS\")/Plink/tutorial/sample_data . \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website Select and exclude lists of samples and SNPs In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click ) Exploring Data To begin the tutorial please navigate to: cd ~/prsworkshop/preworkshop_materials_(\"yourOS\")/Plink/tutorial/sample_data First let's make sure we can call plink from this directory: ../../code/plink Next lets observe the files in the sample data directory: ls We should see the following four files: D1D.ped, D1D.map, D1D.pcs1234, D1D.pheno1 . We first convert the \"old\" format ped/map files to the more memory efficient binary format using the following command: ../../code/plink --file D1D --make-bed --out D1D This generates three new files, D1D.bim, D1D.fam, D1D.bed . Type ls -l , compare how much disk space the bim/fam/bed and ped/map files use. Let's look at the following files by typing the following commands and pressing q to quit after each one: less D1D.bim # Marker / SNP information less D1D.fam # Individual information: IDs less D1D.pcs1234 # A PCA file that lists individuals first four prinicipal components less D1D.pheno1 # A phenotype file that lists individuals phenotypes D1D.bed is a binary file and stores the genotype do not open this file . Investigate the format of the bim and fam files here https://zzz.bwh.harvard.edu/plink/data.shtml#bed, scroll up for details. What do you observe? - What are columns 1, 2, 4, 5, 6 of the bim file? - What are the columns of the fam file? Recoding alleles as counts Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: ../../code/plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be? PLINK website Go to the plink website and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Input filtering', read the different ways SNPs can be filtered. Write SNP list and extract SNPs The --write-snplist writes a list of SNPs (penultimate argument in 'Data Management'). Use this command along with the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!! Performing QC & GWAS (Part II) Here we will work on the following skills: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS Generate summaries to perform QC There are many kinds of summaries of the data that can be generated by PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC. Individual missingness Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"? SNP Missingness Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful? Hardy-Weinberg Equilibrium Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why? Allele frequencies Generate allele frequencies using the command *--*freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way Apply QC filters There are di\ufb00erent strategies for performing QC on your data: (a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice. Apply individual missingness thresholds Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening? Apply SNP missingness and MAF thresholds Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness? Apply Hardy-Weinberg thresholds Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP. Perform GWAS Case/Control GWAS - no covariates Run the following code, which performs a genetic association study using logistic regression on some case/control data: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations? Case/Control GWAS - with covariates Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Plink Tutorial"},{"location":"tut_plink/#introduction-to-plink-part-i","text":"PLINK is the most popular software program for performing genome-wide association analyses, it is extremely extensive allowing a huge number of analyses to be performed. It also includes many options for reformatting your data and provides useful data summaries. Software packages are usually best learnt by having a go at running some of their basic applications and progressing from there (rather than reading the entire user manual first!) - so we begin by running some basic PLINK commands and then work steadily towards performing more sophisticated analyses through these PLINK tutorials.","title":"Introduction to PLINK  (Part I)"},{"location":"tut_plink/#sample-data","text":"This tutorial runs on the data in the pre-workshop materials downloaded here . If you have followed the previous directions, this data should be in: ~/prsworkshop/preworkshop_materials_(\"your OS\")/Plink/tutorial/sample_data . \u26a0\ufe0f All data used in this workshop are simulated . They have no specific biological meaning and are for demonstration purposes only. After completing this practical, you should be able to: Explore and generate genetic data sets needed for GWAS Recode and reorder allelic data Use the PLINK website Select and exclude lists of samples and SNPs In all of the instructions below: - Anything in between the symbols \\<> needs to be changed in some way. For example, \\<file_name> indicates that you should replace that entire statement (including the \\<> symbols) with the appropriate file name. - Bold indicates non- command-line instructions (e.g. right-click )","title":"Sample Data"},{"location":"tut_plink/#exploring-data","text":"To begin the tutorial please navigate to: cd ~/prsworkshop/preworkshop_materials_(\"yourOS\")/Plink/tutorial/sample_data First let's make sure we can call plink from this directory: ../../code/plink Next lets observe the files in the sample data directory: ls We should see the following four files: D1D.ped, D1D.map, D1D.pcs1234, D1D.pheno1 . We first convert the \"old\" format ped/map files to the more memory efficient binary format using the following command: ../../code/plink --file D1D --make-bed --out D1D This generates three new files, D1D.bim, D1D.fam, D1D.bed . Type ls -l , compare how much disk space the bim/fam/bed and ped/map files use. Let's look at the following files by typing the following commands and pressing q to quit after each one: less D1D.bim # Marker / SNP information less D1D.fam # Individual information: IDs less D1D.pcs1234 # A PCA file that lists individuals first four prinicipal components less D1D.pheno1 # A phenotype file that lists individuals phenotypes D1D.bed is a binary file and stores the genotype do not open this file . Investigate the format of the bim and fam files here https://zzz.bwh.harvard.edu/plink/data.shtml#bed, scroll up for details. What do you observe? - What are columns 1, 2, 4, 5, 6 of the bim file? - What are the columns of the fam file?","title":"Exploring Data"},{"location":"tut_plink/#recoding-alleles-as-counts","text":"Genotype data in allele count format is very useful, for example to use in regression modelling in statistical software such as R. Generate the D1D data in allele count format: ../../code/plink --bfile D1D --recodeA --out D1D_AC \ud83d\udcdd There are several options for recoding SNPs in different ways - more information on the PLINK website (see next section). Again note that a log file was created - skim the log file or screen output Look inside the .raw file. What do you think the 0/1/2 represent? Do there appear to be more 0s or 2s? Why might this be?","title":"Recoding alleles as counts"},{"location":"tut_plink/#plink-website","text":"Go to the plink website and skim through the front page to get an idea of PLINK's functionality. Note the list of clickable links on the left side of the website. Under 'Data Management' (click the heading on the left) and read the list of the di\ufb00erent ways you may want to recode and reorder data sets. Don't attempt to read much further as this is a very large and detailed section - a useful future resource but too much for today. Under 'Input filtering', read the different ways SNPs can be filtered.","title":"PLINK website"},{"location":"tut_plink/#write-snp-list-and-extract-snps","text":"The --write-snplist writes a list of SNPs (penultimate argument in 'Data Management'). Use this command along with the information that you found on the PLINK website to create a command to extract a list of SNPs. Below is a list of requirements - try to do this before you go to the end of this section, where the full command is given and explained. Set the D1D binary file as input Set MAF threshold to 0.05 Set SNP missingness threshold to 0.05 Add the appropriate command to write out a snp list containing only those SNPs with MAF above 0.05 and missingness below 0.05 Use 'D1D_snps' as the output file name After the command has run, check the output for your SNP list and look at it with the default viewer. You will now use the SNP list that you have created to extract those SNPs and create a new set of data files in a single command. Use the D1D binary file set as input Find the command for extracting a set of SNPs listed in a file (hint: Data Management section) and combine it with a command that you learned above to create binary files Use the output file name 'D1D_MAF_MISS' \ud83d\udcdd Log files are uselful to check that the number of SNPs and samples is as expected. Always check your your log files to ensure that they are sensible. SNP lists can also be used to EXCLUDE SNPs - select 'exclude' above instead of 'extract'. Sample ID lists can also be used to 'keep' or 'remove' individuals in the same 'filter' window. Note that both sample IDs (FID IID,separated by a space are required in the sample file list. Solution 1: TO BE REVEALED LATER!! Solution 2: TO BE REVEALED LATER!!","title":"Write SNP list and extract SNPs"},{"location":"tut_plink/#performing-qc-gwas-part-ii","text":"Here we will work on the following skills: Generate summaries of the data needed for QC Apply QC thresholds Perform GWAS","title":"Performing QC &amp; GWAS (Part II)"},{"location":"tut_plink/#generate-summaries-to-perform-qc","text":"There are many kinds of summaries of the data that can be generated by PLINK in order to perform particular quality control (QC) steps, which help to make our data more reliable. Some of these involve summaries in relation to the individuals (e.g. individual missingness, sex-check) and some relate to summaries of SNP data (e.g. MAF, Hardy-Weinburg Equilibrium). Over the next few sub-sections you will go through some examples of generating summary statistics that can be used to perform QC.","title":"Generate summaries to perform QC"},{"location":"tut_plink/#individual-missingness","text":"Use the D1D binary files to generate files containing missingness information (--missing). Use the output file name 'D1D_miss' Open the 2 files that were generated (lmiss & imiss). What do the two output files contain? In the imiss file, what is the meaning of the data in the column headed \"F_MISS\"?","title":"Individual missingness"},{"location":"tut_plink/#snp-missingness","text":"Look inside the file containing SNP missingness information: D1D_miss.lmiss. What is the meaning of the value under F_MISS? What does the command --test-missing do and why might it be useful?","title":"SNP Missingness"},{"location":"tut_plink/#hardy-weinberg-equilibrium","text":"Generate HWE statistics using the --hardy option. Use output file name D1D_hardy. Open and examine results. Why are there multiple rows for each SNP and what does each mean? Which of the rows do you think should be used to exclude SNPs from the subsequent analysis (if any) for failing the HWE test? Why?","title":"Hardy-Weinberg Equilibrium"},{"location":"tut_plink/#allele-frequencies","text":"Generate allele frequencies using the command *--*freq. Use D1D_freq as the output name. Examine the output. What is the heading of the column that tells you which nucleotide is the minor allele? \ud83d\udcdd This information is important to remember as many PLINK files use this notation. The minor allele is always labeled the same way","title":"Allele frequencies"},{"location":"tut_plink/#apply-qc-filters","text":"","title":"Apply QC filters"},{"location":"tut_plink/#there-are-different-strategies-for-performing-qc-on-your-data","text":"(a) Create lists of SNPs and individuals and use --remove, --extract, --exclude, --include to create new file sets (good for documentation, collaboration) (b) Apply thresholds one at a time and generate new bed/bim/fam file (good for applying sequential filters) (c) Use options (e.g. --maf ) in other commands (e.g. --assoc) to remove SNPs or samples at required QC thresholds during analysis. \ud83d\udcdd We have already seen how to select or exclude individuals or SNPs by first creating lists (a), so in this section we will set thresholds to generate new files sets in a single command. However, it is useful to have lists of all SNPs and individuals excluded pre-analysis, according to the reason for exclusion, so generating and retaining such files using the techniques that we used before for good practice.","title":"There are di\ufb00erent strategies for performing QC on your data:"},{"location":"tut_plink/#apply-individual-missingness-thresholds","text":"Generate new binary file sets (--make-bed) from the 'D1D' binary file set, removing individuals with missingness greater than 3% using a single command (hint: In the 'Inclusion thresholds' section, see the 'Missing/person' sub-section). Use the output file name 'D1D_imiss3pc' Examine the output files (no need to open, and remember the bed file cannot be read) and the log file How many individuals were in the original file? How many individuals were removed? How many males and females were left after screening?","title":"Apply individual missingness thresholds"},{"location":"tut_plink/#apply-snp-missingness-and-maf-thresholds","text":"Create new binary file sets from the 'D1D_imiss3pc' binary file set (NOT the original D1D files) by setting MAF threshold to 0.05 and SNP missingness threshold to 0.02 (See 'Inclusion thresholds' to obtain the correct threshold flags). Use the output file name'D1D_imiss3pc_lmiss2pc_maf5pc Examine the output files and the log file How many SNPs were in the original files? How many SNPs were removed for low minor allele frequency? How many SNPs were removed for missingness?","title":"Apply SNP missingness and MAF thresholds"},{"location":"tut_plink/#apply-hardy-weinberg-thresholds","text":"Generate a new binary file set called 'D1D_QC' from the D1D_imiss3pc_lmiss2pc_maf5pc file, applying a HWE threshold of 0.0001. This is our final, QC'ed file set. Examine log and output files. -How many SNPs were removed for HWE p-values below the threshold? \ud83d\udcdd It is useful to know how to do this, but be careful about setting this threshold - strong association signals can cause departure from HWE and you may remove great results! Use a lenient threshold and apply to controls only to avoid this problem. HWE can also be checked post-hoc for each SNP.","title":"Apply Hardy-Weinberg thresholds"},{"location":"tut_plink/#perform-gwas","text":"","title":"Perform GWAS"},{"location":"tut_plink/#casecontrol-gwas-no-covariates","text":"Run the following code, which performs a genetic association study using logistic regression on some case/control data: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --out D1D_CC What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Are there any other significant associations?","title":"Case/Control GWAS - no covariates"},{"location":"tut_plink/#casecontrol-gwas-with-covariates","text":"Here we repeat the previous analysis but this time including some covariates. The file D1D.pcs1234 contains the first 4 principal components from a PCA on the genetic data. Run the analysis specifying the covariates file: ../../code/plink --bfile D1D_QC --logistic --adjust --pheno D1D.pheno1 --covar D1D.pcs1234 --out D1D_CC_PCadj What are the raw and Bonferroni-adjusted p-values for the top hit? What does this mean - is there a significant association? Suggest a reason for the different results when adjusting for the 4 PCs?","title":"Case/Control GWAS - with covariates"},{"location":"tut_python/","text":"Introduction to Python Python is a useful general purpose programming language. Here we will go over some basic python commands from an interactive shell. To being open python3 by typing: python3 Libraries Downloaded python libraries can be imported interactively: import matplotlib.pyplot as plt Variables in Python You can assign a value or values to any variable you want using the equals sign: X = 5 # X is an integer (5) X = [1,2,3,4,5] # X is a list containing the first five integers Functions can also be used: X = range(-10,11,1) # X includes all integers between -10 and 10 counting by 1 New variables can be declared with lists: Y = [x*x for x in X] # Y is equal to X squared A figure can be generated: plt.plot(X,Y) And viewed: plt.show()","title":"Python Tutorial"},{"location":"tut_python/#introduction-to-python","text":"Python is a useful general purpose programming language. Here we will go over some basic python commands from an interactive shell. To being open python3 by typing: python3","title":"Introduction to Python"},{"location":"tut_python/#libraries","text":"Downloaded python libraries can be imported interactively: import matplotlib.pyplot as plt","title":"Libraries"},{"location":"tut_python/#variables-in-python","text":"You can assign a value or values to any variable you want using the equals sign: X = 5 # X is an integer (5) X = [1,2,3,4,5] # X is a list containing the first five integers Functions can also be used: X = range(-10,11,1) # X includes all integers between -10 and 10 counting by 1 New variables can be declared with lists: Y = [x*x for x in X] # Y is equal to X squared A figure can be generated: plt.plot(X,Y) And viewed: plt.show()","title":"Variables in Python"},{"location":"practicals/workshop_practical_clive/","text":"Day 2 Morning practical: Cross-ancestry PRS analyses using BridgePRS Table of contents Key learning outcomes BridgePRS input data Passing arguments to BridgePRS - example run BridgePRS output Using BridgePRS without target summary statistics Further analyses with BridgePRS Using BridgePRS SNP weights Key learning outcomes In the previous lecture we covered the modelling used by BridgePRS. Here we will use the BridgePRS software to apply the method. The aim of this practical is to provide you with a basic understanding and some experience of running BridgePRS software. After completing this practical, you should be able to: perform cross-population analyses using BridgePRS understand what input files are required by BridgePRS set up configuration files to pass arguments to BridgePRS interpret output from BridgePRS use BridgePRS output to calculate PRS in unseen samples using SNP weights Back to table of contents BridgePRS input data In the BridgePRS directory there is a data folder which we will use in this practical. View the data directory $ ls -l data total 5368 drwxr-xr-x 73 hoggac01 staff 2336 26 Jul 16:00 1000G_sample -rw-r--r-- 1 hoggac01 staff 469 12 Aug 14:35 afr.config -rw-r--r-- 1 hoggac01 staff 469 12 Aug 14:35 eas.config -rw-r--r-- 1 hoggac01 staff 410 12 Aug 14:35 eur.config drwxr-xr-x 5 hoggac01 staff 160 14 Jul 17:22 pop_AFR drwxr-xr-x 5 hoggac01 staff 160 14 Jul 17:22 pop_EAS drwxr-xr-x 5 hoggac01 staff 160 7 Aug 20:30 pop_EUR -rw-r--r-- 1 hoggac01 staff 200376 7 Aug 21:30 qc_snplist.txt The pop_AFR, pop_EAS, pop_EUR folders contain simulated genotype, phenotype and GWAS summary statistics representative of Europeans, East Asians and Africans for input to BridgePRS. Each pop_* folder is split into summary statistics, and individual level genotype and phenotype folders, e.g. $ ls -l data/pop_AFR/ total 0 drwxr-xr-x 68 hoggac01 staff 2176 14 Jul 17:22 genotypes drwxr-xr-x 4 hoggac01 staff 128 14 Jul 17:22 phenotypes drwxr-xr-x 68 hoggac01 staff 2176 12 Aug 11:02 sumstats Look at each directory e.g. ls pop_AFR/genotypes . There are two sets of summary statistics in each sumstats folder from the analysis of the same simulated continuous phenotype, the \"half\" files were generated using half the sample size. For computation speed the summary statistics only have a small subset of SNPs, 19k-20k genomewide, the following commands count the number of lines in each set of summary statistics zcat data/pop_EAS/sumstats/EAS.chr* | wc -l zcat data/pop_EUR/sumstats/EUR.chr* | wc -l zcat data/pop_AFR/sumstats/AFR.chr* | wc -l or on a Mac gzcat data/pop_EAS/sumstats/EAS.chr* | wc -l gzcat data/pop_EUR/sumstats/EUR.chr* | wc -l gzcat data/pop_AFR/sumstats/AFR.chr* | wc -l zcat / gzcat are used as the files are gzipped. | \"pipes\" the output to wc -l which counts the number of lines. Results in these files are only shown for SNPs with MAF>0 and are a random subset of HapMap SNPs. Questions? Which population has the most polymorphic SNPs? Which population has the least polymorphic SNPs? Do you think there is a bias in the SNP ascertainment? Which population would you expect to have the most polymorphic SNPs if SNPs were ascertained randomly from 1000G? Take a look a look at the files, e.g. zcat data/pop_AFR/sumstats/AFR.chr19.glm.linear.gz | head zcat data/pop_AFR/sumstats/AFR_half.chr19.glm.linear.gz | head again use gzcat on a Mac. In the OBS_CT column you'll see can that the \"_half\" summary statistics files have half the sample size, 10,000 compared to 20,000 for the EAS and AFR populations and 40,000 compared to 80,000 for the EUR population. The same SNPs are contained in each set of summary statistics. The phenotypes folders has two files: \"test\" and \"validation\" with IDs, the outcome phenotype and covariates. \"Test\" data is used to estimate the PRS and \"validation\" data is used only to assess model performance, it is not used to estimate the PRS. The genotypes folders are in plink binary format and are split by chromosome. These folders contain the genetic data for both test and validation individuals in the phenotypes folder. Test data is only used for individuals with both genotype and phenotype information. Similarly model performance metrics is only used validation samples with both genotype and phenotype information, however, predictions are generated for all validation samples with genotype data. Back to table of contents Passing arguments to BridgePRS - example run Example run of BridgePRS: ./bridgePRS pipeline go -o out/ --config_files data/eas.config data/eur.config --fst 0.11 --phenotype y --cores 4 --restart Arguments can be passed to BridgePRS both directly on the command-line and in config files. The command-line arguments used above are: * -o Output folder * --config_files Configuration files, see below * --fst Fst between the base and target populations used in the analysis, used to specify a prior distribution. The Fst between EAS and EUR is 0.11 * --phenotype The column label of the phenotype in the test and validation files, e.g. EAS_valid.dat * --cores The number of cores used in the analysis. Fixation index Fst Fst is a measure of genetic distance between two populations. Details on the definition and calculation of Fst can be found on Wikipedia . Given genetic data from two populations Fst can be calculated in plink , for details see https://www.cog-genomics.org/plink/1.9/basic_stats#fst. The *.config files config files are a neat way to store population specific arguments. For a standard two population analysis two config files are required, by default the first config file is for the target population and the second is for the base population. .config files tell BridgePRS where to find the required input files and the column headers of the summary statistics files for a population data set, take a look, e.g. cat data/eas.config POP=EAS LDPOP=EAS LD_PATH=1000G_sample SUMSTATS_PREFIX=pop_EAS/sumstats/EAS.chr SUMSTATS_SIZE=20000 #SUMSTATS_PREFIX=pop_EAS/sumstats/EAS_half.chr #SUMSTATS_SIZE=10000 SUMSTATS_SUFFIX=.glm.linear.gz GENOTYPE_PREFIX=pop_EAS/genotypes/chr PHENOTYPE_FILE=pop_EAS/phenotypes/EAS_test.dat VALIDATION_FILE=pop_EAS/phenotypes/EAS_valid.dat SNP_FILE=qc_snplist.txt SSF-P=P SSF-SNPID=ID SSF-BETA=BETA SSF-REF=REF SSF-ALT=A1 This config file contains all possible arguments that can be used in config files. config files use the same argument names as the command-line arguments but in uppercase, and use \"=\" instead of a space between the argument name and the argument being passed. The POP argument simply labels the population used in this .config file for output. A full list of arguments can be found here . Estimating linkage disequilibrium (LD) BridgePRS requires individual level genetic data in plink binary format to estimate linkage disequilibrium (LD) in the populations which produced the GWAS summary statistics. The genotype test and validation data could be used, e.g. data here pop_EUR/genotypes/ . If these data are small, less than 500 samples, or are not representative of the GWAS population, we provide 1000 Genomes (1000G) data to estimate LD. Suitable 1000G data for this analysis is in the 1000G_sample folder for the small subset of SNPs used in these examples. The .config files point to the folder with reference LD data by the LD_PATH argument in the config file. LD reference data is available for the five 1000G super populations (abbreviations required to use in brackets): East Asian (EAS), South Asian (SAS), European (Eur), African (AFR) and American (AMR). For real data analyses 1000G reference data for larger subsets of SNPs can be downloaded here Questions? Can you work out what the other command line arguments are doing? Can you work out what the other config file arguments are doing? Back to table of contents BridgePRS output The main output is in the folder out/prs-combined_EAS-EUR/ . First view the output summary plot evince out/prs-combined_EAS-EUR/bridgePRS-combo.pdf on a Mac use open instead of evince . The barplot at the top shows the variance explained (R2) by the four PRS models BridgePRS estimates. The weighted model is BridgePRS estimated \"best\" PRS. The models estimated by BridgePRS The three separate target population PRS estimated by BridgePRS are: * Stage 2 model -- estimated using target population data with prior effect-size distribution from the base (European) population * Stage 1 model -- estimated using only the target (Non-European) data * Stage1+2 model -- estimated using both stage 1 and stage 2 models Each of these three models are given weights corresponding to how well they fit the test data. These weights are then used to combine the PRS to give the single weighted combined PRS. The weighted combined PRS should typically be used. The models, stage1, stage2 and stage1+2, should not be used unless users have a strong prior belief that a particular model is better. The hypotheses of the three models are: * Stage 2 model reflects the belief that the target population GWAS is only informative when used with the base population GWAS * Stage 1 model reflects the belief that the target population GWAS is informative and the base population GWAS gives no addition information. * Stage 1+2 model reflects the belief both the base and target population GWAS contribute independent information Look at the following output file cat out/prs-combined_EAS-EUR/EAS_weighted_combined_var_explained.txt Questions? Which plot in the summary plot was constructed from this output file? How do the Manhattan plots of the base and target populations compare? EAS_weighted_combined_preds.dat has PRS predictions for samples in the validation data using all four models: stage1, stage2, stage1+2 and weighted. EAS_weighted_combined_snp_weights.dat has the SNP weights for the combined to allow this model to be applied to other samples. Back to table of contents Using BridgePRS without target summary statistics Often GWAS summary statistics are only available in one population. BridgePRS can use these summary statistics and optimise them to estimate a PRS for another target population given individual level from the target population. Here is an example using the data/eur_eas.config config file ./bridgePRS prs-single run -o out_single/ --config_file data/eur_eas.config --phenotype y --cores 4 Look at data/eur_eas.config . Questions? What GWAS summary data is used? What test data is used for model optimisation? What validation data is used? Results of interest are written to the folder out_single/prs-single_EAS/quantify/ . Model performance is shown in the file EAS_quantify_var_explained.txt and plotted in .... See how these results compare with the previous analysis which included EAS GWAS summary statistics. cat out_single/prs-single_EAS/quantify/EAS_quantify_var_explained.txt cat out/prs-combined_EAS-EUR/EAS_weighted_combined_var_explained.txt This single summary statistic analysis is equivalent to the stage 2 analysis previously but with all the weight on the EUR prior. Question? How does prediction using only EUR summary statistics compare with those which include information from the EAS summary statistics? Back to table of contents Further analyses with BridgePRS African analysis Run BridgePRS again to estimate PRS in Africans using afr.config . Note, you should also change the command line fst argument to match the Fst between Africans and Europeans, use 0.15. Qustions? Which population, EAS or AFR, has the best prediction? What are the reasons for the differences in prediction between the populations? Analyses with other GWAS summary statistics For each population the config files contain commented out links to GWAS summary statistics of the same phenotype using half the same size: 40k for EUR and 10k for both EAS and AFR. Edit eas.config to use the EAS 10k GWAS summary statistics. To run the analysis write results to a new output directory e.g. out_half_target . Run the similar analysis for African samples by editing afr.config . Compare with previous results using the 10k EAS and EAS GWAS. Compare EAS and AFR results. Check you've run the analyses using the correct GWAS summary statistics, e.g. less less out_half_target/logs/bridgePRS.eas-eur.pipeline.go.log less less out_half_target/logs/bridgePRS.afr-eur.pipeline.go.log or grep Sumstats out_half_target/logs/bridgePRS.eas-eur.pipeline.go.log grep Sumstats out_half_target/logs/bridgePRS.afr-eur.pipeline.go.log If you have made a mistake, correct and run again using the --restart flag which deletes the previously generated results. Questions? How has using the less well powered EAS and AFR GWAS affected the predictive accuracy of the BridgePRS models? How do AFR and EAS results compare? Analyses with smaller EUR GWAS summary statistics Run BridgePRS analysis using using the 40k EUR GWAS (use EUR_half ) as base and 20k EAS GWAS as target. You will need to edit eur.config and eas.config to use the correct summary statistics and pass the correct GWAS sample size. Write to results to a new directory e.g. out_half_eur . Run the equivalent analysis for AFR. Questions? How has using the less well powered EUR GWAS affected the predictive accuracy of the BridgePRS models? How do AFR and EAS results compare? Back to table of contents Using BridgePRS SNP weights The SNP weights in EAS_weighted_combined_snp_weights.dat can be used to make predictions in other samples for which we have overlapping genotype data. We demonstrate this using plink and data in data/pop_EAS/genotypes/ . The genotype data is split by chromosome, therefore predictions are estimated for each chromosome separately and then combined. The following bash commands estimate the per chromosome predictions using plink, we first make a directory to write these predictions to mkdir out/preds for chr in {1..22} do plink --score out/prs-combined_EAS-EUR/EAS_weighted_combined_snp_weights.dat 1 2 4 sum \\ --bfile data/pop_EAS/genotypes/chr$chr \\ --out out/preds/EAS_chr$chr done We then read these predictions into R to combine R library(data.table) tmp <- fread(paste0('out/preds/EAS_chr1.profile')) pred <- data.frame( tmp$IID, tmp$SCORESUM ) colnames(pred) <- c('IID','Score') for( chr in 2:22 ){ tmp <- fread(paste0('out/preds/EAS_chr',chr,'.profile')) pred[,2] <- pred[,2] + tmp$SCORESUM } # Check predictions are the same as those produced directly by BridgePRS valid <- fread('out/prs-combined_EAS-EUR/EAS_weighted_combined_preds.dat') # Match IDs ptr <- match( valid$IID, pred$IID ) plot( pred$Score[ptr], valid$Weighted, main=\"Individual PRS\" xlab=\"PRS estimated by BridgePRS\", ylab=\"PRS re-estimated from SNP weights\" ) Predictions are the same +/- rounding error and a constant. Back to table of contents","title":"Day2 (AM)"},{"location":"practicals/workshop_practical_clive/#day-2-morning-practical-cross-ancestry-prs-analyses-using-bridgeprs","text":"","title":"Day 2 Morning practical: Cross-ancestry PRS analyses using BridgePRS"},{"location":"practicals/workshop_practical_clive/#table-of-contents","text":"Key learning outcomes BridgePRS input data Passing arguments to BridgePRS - example run BridgePRS output Using BridgePRS without target summary statistics Further analyses with BridgePRS Using BridgePRS SNP weights","title":"Table of contents"},{"location":"practicals/workshop_practical_clive/#key-learning-outcomes","text":"In the previous lecture we covered the modelling used by BridgePRS. Here we will use the BridgePRS software to apply the method. The aim of this practical is to provide you with a basic understanding and some experience of running BridgePRS software. After completing this practical, you should be able to: perform cross-population analyses using BridgePRS understand what input files are required by BridgePRS set up configuration files to pass arguments to BridgePRS interpret output from BridgePRS use BridgePRS output to calculate PRS in unseen samples using SNP weights Back to table of contents","title":"Key learning outcomes"},{"location":"practicals/workshop_practical_clive/#bridgeprs-input-data","text":"In the BridgePRS directory there is a data folder which we will use in this practical. View the data directory $ ls -l data total 5368 drwxr-xr-x 73 hoggac01 staff 2336 26 Jul 16:00 1000G_sample -rw-r--r-- 1 hoggac01 staff 469 12 Aug 14:35 afr.config -rw-r--r-- 1 hoggac01 staff 469 12 Aug 14:35 eas.config -rw-r--r-- 1 hoggac01 staff 410 12 Aug 14:35 eur.config drwxr-xr-x 5 hoggac01 staff 160 14 Jul 17:22 pop_AFR drwxr-xr-x 5 hoggac01 staff 160 14 Jul 17:22 pop_EAS drwxr-xr-x 5 hoggac01 staff 160 7 Aug 20:30 pop_EUR -rw-r--r-- 1 hoggac01 staff 200376 7 Aug 21:30 qc_snplist.txt The pop_AFR, pop_EAS, pop_EUR folders contain simulated genotype, phenotype and GWAS summary statistics representative of Europeans, East Asians and Africans for input to BridgePRS. Each pop_* folder is split into summary statistics, and individual level genotype and phenotype folders, e.g. $ ls -l data/pop_AFR/ total 0 drwxr-xr-x 68 hoggac01 staff 2176 14 Jul 17:22 genotypes drwxr-xr-x 4 hoggac01 staff 128 14 Jul 17:22 phenotypes drwxr-xr-x 68 hoggac01 staff 2176 12 Aug 11:02 sumstats Look at each directory e.g. ls pop_AFR/genotypes . There are two sets of summary statistics in each sumstats folder from the analysis of the same simulated continuous phenotype, the \"half\" files were generated using half the sample size. For computation speed the summary statistics only have a small subset of SNPs, 19k-20k genomewide, the following commands count the number of lines in each set of summary statistics zcat data/pop_EAS/sumstats/EAS.chr* | wc -l zcat data/pop_EUR/sumstats/EUR.chr* | wc -l zcat data/pop_AFR/sumstats/AFR.chr* | wc -l or on a Mac gzcat data/pop_EAS/sumstats/EAS.chr* | wc -l gzcat data/pop_EUR/sumstats/EUR.chr* | wc -l gzcat data/pop_AFR/sumstats/AFR.chr* | wc -l zcat / gzcat are used as the files are gzipped. | \"pipes\" the output to wc -l which counts the number of lines. Results in these files are only shown for SNPs with MAF>0 and are a random subset of HapMap SNPs.","title":"BridgePRS input data"},{"location":"practicals/workshop_practical_clive/#questions","text":"Which population has the most polymorphic SNPs? Which population has the least polymorphic SNPs? Do you think there is a bias in the SNP ascertainment? Which population would you expect to have the most polymorphic SNPs if SNPs were ascertained randomly from 1000G? Take a look a look at the files, e.g. zcat data/pop_AFR/sumstats/AFR.chr19.glm.linear.gz | head zcat data/pop_AFR/sumstats/AFR_half.chr19.glm.linear.gz | head again use gzcat on a Mac. In the OBS_CT column you'll see can that the \"_half\" summary statistics files have half the sample size, 10,000 compared to 20,000 for the EAS and AFR populations and 40,000 compared to 80,000 for the EUR population. The same SNPs are contained in each set of summary statistics. The phenotypes folders has two files: \"test\" and \"validation\" with IDs, the outcome phenotype and covariates. \"Test\" data is used to estimate the PRS and \"validation\" data is used only to assess model performance, it is not used to estimate the PRS. The genotypes folders are in plink binary format and are split by chromosome. These folders contain the genetic data for both test and validation individuals in the phenotypes folder. Test data is only used for individuals with both genotype and phenotype information. Similarly model performance metrics is only used validation samples with both genotype and phenotype information, however, predictions are generated for all validation samples with genotype data. Back to table of contents","title":"Questions?"},{"location":"practicals/workshop_practical_clive/#passing-arguments-to-bridgeprs-example-run","text":"Example run of BridgePRS: ./bridgePRS pipeline go -o out/ --config_files data/eas.config data/eur.config --fst 0.11 --phenotype y --cores 4 --restart Arguments can be passed to BridgePRS both directly on the command-line and in config files. The command-line arguments used above are: * -o Output folder * --config_files Configuration files, see below * --fst Fst between the base and target populations used in the analysis, used to specify a prior distribution. The Fst between EAS and EUR is 0.11 * --phenotype The column label of the phenotype in the test and validation files, e.g. EAS_valid.dat * --cores The number of cores used in the analysis.","title":"Passing arguments to BridgePRS - example run"},{"location":"practicals/workshop_practical_clive/#fixation-index-fst","text":"Fst is a measure of genetic distance between two populations. Details on the definition and calculation of Fst can be found on Wikipedia . Given genetic data from two populations Fst can be calculated in plink , for details see https://www.cog-genomics.org/plink/1.9/basic_stats#fst.","title":"Fixation index Fst"},{"location":"practicals/workshop_practical_clive/#the-config-files","text":"config files are a neat way to store population specific arguments. For a standard two population analysis two config files are required, by default the first config file is for the target population and the second is for the base population. .config files tell BridgePRS where to find the required input files and the column headers of the summary statistics files for a population data set, take a look, e.g. cat data/eas.config POP=EAS LDPOP=EAS LD_PATH=1000G_sample SUMSTATS_PREFIX=pop_EAS/sumstats/EAS.chr SUMSTATS_SIZE=20000 #SUMSTATS_PREFIX=pop_EAS/sumstats/EAS_half.chr #SUMSTATS_SIZE=10000 SUMSTATS_SUFFIX=.glm.linear.gz GENOTYPE_PREFIX=pop_EAS/genotypes/chr PHENOTYPE_FILE=pop_EAS/phenotypes/EAS_test.dat VALIDATION_FILE=pop_EAS/phenotypes/EAS_valid.dat SNP_FILE=qc_snplist.txt SSF-P=P SSF-SNPID=ID SSF-BETA=BETA SSF-REF=REF SSF-ALT=A1 This config file contains all possible arguments that can be used in config files. config files use the same argument names as the command-line arguments but in uppercase, and use \"=\" instead of a space between the argument name and the argument being passed. The POP argument simply labels the population used in this .config file for output. A full list of arguments can be found here .","title":"The *.config files"},{"location":"practicals/workshop_practical_clive/#estimating-linkage-disequilibrium-ld","text":"BridgePRS requires individual level genetic data in plink binary format to estimate linkage disequilibrium (LD) in the populations which produced the GWAS summary statistics. The genotype test and validation data could be used, e.g. data here pop_EUR/genotypes/ . If these data are small, less than 500 samples, or are not representative of the GWAS population, we provide 1000 Genomes (1000G) data to estimate LD. Suitable 1000G data for this analysis is in the 1000G_sample folder for the small subset of SNPs used in these examples. The .config files point to the folder with reference LD data by the LD_PATH argument in the config file. LD reference data is available for the five 1000G super populations (abbreviations required to use in brackets): East Asian (EAS), South Asian (SAS), European (Eur), African (AFR) and American (AMR). For real data analyses 1000G reference data for larger subsets of SNPs can be downloaded here","title":"Estimating linkage disequilibrium (LD)"},{"location":"practicals/workshop_practical_clive/#questions_1","text":"Can you work out what the other command line arguments are doing? Can you work out what the other config file arguments are doing? Back to table of contents","title":"Questions?"},{"location":"practicals/workshop_practical_clive/#bridgeprs-output","text":"The main output is in the folder out/prs-combined_EAS-EUR/ . First view the output summary plot evince out/prs-combined_EAS-EUR/bridgePRS-combo.pdf on a Mac use open instead of evince . The barplot at the top shows the variance explained (R2) by the four PRS models BridgePRS estimates. The weighted model is BridgePRS estimated \"best\" PRS.","title":"BridgePRS output"},{"location":"practicals/workshop_practical_clive/#the-models-estimated-by-bridgeprs","text":"The three separate target population PRS estimated by BridgePRS are: * Stage 2 model -- estimated using target population data with prior effect-size distribution from the base (European) population * Stage 1 model -- estimated using only the target (Non-European) data * Stage1+2 model -- estimated using both stage 1 and stage 2 models Each of these three models are given weights corresponding to how well they fit the test data. These weights are then used to combine the PRS to give the single weighted combined PRS. The weighted combined PRS should typically be used. The models, stage1, stage2 and stage1+2, should not be used unless users have a strong prior belief that a particular model is better. The hypotheses of the three models are: * Stage 2 model reflects the belief that the target population GWAS is only informative when used with the base population GWAS * Stage 1 model reflects the belief that the target population GWAS is informative and the base population GWAS gives no addition information. * Stage 1+2 model reflects the belief both the base and target population GWAS contribute independent information Look at the following output file cat out/prs-combined_EAS-EUR/EAS_weighted_combined_var_explained.txt","title":"The models estimated by BridgePRS"},{"location":"practicals/workshop_practical_clive/#questions_2","text":"Which plot in the summary plot was constructed from this output file? How do the Manhattan plots of the base and target populations compare? EAS_weighted_combined_preds.dat has PRS predictions for samples in the validation data using all four models: stage1, stage2, stage1+2 and weighted. EAS_weighted_combined_snp_weights.dat has the SNP weights for the combined to allow this model to be applied to other samples. Back to table of contents","title":"Questions?"},{"location":"practicals/workshop_practical_clive/#using-bridgeprs-without-target-summary-statistics","text":"Often GWAS summary statistics are only available in one population. BridgePRS can use these summary statistics and optimise them to estimate a PRS for another target population given individual level from the target population. Here is an example using the data/eur_eas.config config file ./bridgePRS prs-single run -o out_single/ --config_file data/eur_eas.config --phenotype y --cores 4 Look at data/eur_eas.config .","title":"Using BridgePRS without target summary statistics"},{"location":"practicals/workshop_practical_clive/#questions_3","text":"What GWAS summary data is used? What test data is used for model optimisation? What validation data is used? Results of interest are written to the folder out_single/prs-single_EAS/quantify/ . Model performance is shown in the file EAS_quantify_var_explained.txt and plotted in .... See how these results compare with the previous analysis which included EAS GWAS summary statistics. cat out_single/prs-single_EAS/quantify/EAS_quantify_var_explained.txt cat out/prs-combined_EAS-EUR/EAS_weighted_combined_var_explained.txt This single summary statistic analysis is equivalent to the stage 2 analysis previously but with all the weight on the EUR prior.","title":"Questions?"},{"location":"practicals/workshop_practical_clive/#question","text":"How does prediction using only EUR summary statistics compare with those which include information from the EAS summary statistics? Back to table of contents","title":"Question?"},{"location":"practicals/workshop_practical_clive/#further-analyses-with-bridgeprs","text":"","title":"Further analyses with BridgePRS"},{"location":"practicals/workshop_practical_clive/#african-analysis","text":"Run BridgePRS again to estimate PRS in Africans using afr.config . Note, you should also change the command line fst argument to match the Fst between Africans and Europeans, use 0.15.","title":"African analysis"},{"location":"practicals/workshop_practical_clive/#qustions","text":"Which population, EAS or AFR, has the best prediction? What are the reasons for the differences in prediction between the populations?","title":"Qustions?"},{"location":"practicals/workshop_practical_clive/#analyses-with-other-gwas-summary-statistics","text":"For each population the config files contain commented out links to GWAS summary statistics of the same phenotype using half the same size: 40k for EUR and 10k for both EAS and AFR. Edit eas.config to use the EAS 10k GWAS summary statistics. To run the analysis write results to a new output directory e.g. out_half_target . Run the similar analysis for African samples by editing afr.config . Compare with previous results using the 10k EAS and EAS GWAS. Compare EAS and AFR results. Check you've run the analyses using the correct GWAS summary statistics, e.g. less less out_half_target/logs/bridgePRS.eas-eur.pipeline.go.log less less out_half_target/logs/bridgePRS.afr-eur.pipeline.go.log or grep Sumstats out_half_target/logs/bridgePRS.eas-eur.pipeline.go.log grep Sumstats out_half_target/logs/bridgePRS.afr-eur.pipeline.go.log If you have made a mistake, correct and run again using the --restart flag which deletes the previously generated results.","title":"Analyses with other GWAS summary statistics"},{"location":"practicals/workshop_practical_clive/#questions_4","text":"How has using the less well powered EAS and AFR GWAS affected the predictive accuracy of the BridgePRS models? How do AFR and EAS results compare?","title":"Questions?"},{"location":"practicals/workshop_practical_clive/#analyses-with-smaller-eur-gwas-summary-statistics","text":"Run BridgePRS analysis using using the 40k EUR GWAS (use EUR_half ) as base and 20k EAS GWAS as target. You will need to edit eur.config and eas.config to use the correct summary statistics and pass the correct GWAS sample size. Write to results to a new directory e.g. out_half_eur . Run the equivalent analysis for AFR.","title":"Analyses with smaller EUR GWAS summary statistics"},{"location":"practicals/workshop_practical_clive/#questions_5","text":"How has using the less well powered EUR GWAS affected the predictive accuracy of the BridgePRS models? How do AFR and EAS results compare? Back to table of contents","title":"Questions?"},{"location":"practicals/workshop_practical_clive/#using-bridgeprs-snp-weights","text":"The SNP weights in EAS_weighted_combined_snp_weights.dat can be used to make predictions in other samples for which we have overlapping genotype data. We demonstrate this using plink and data in data/pop_EAS/genotypes/ . The genotype data is split by chromosome, therefore predictions are estimated for each chromosome separately and then combined. The following bash commands estimate the per chromosome predictions using plink, we first make a directory to write these predictions to mkdir out/preds for chr in {1..22} do plink --score out/prs-combined_EAS-EUR/EAS_weighted_combined_snp_weights.dat 1 2 4 sum \\ --bfile data/pop_EAS/genotypes/chr$chr \\ --out out/preds/EAS_chr$chr done We then read these predictions into R to combine R library(data.table) tmp <- fread(paste0('out/preds/EAS_chr1.profile')) pred <- data.frame( tmp$IID, tmp$SCORESUM ) colnames(pred) <- c('IID','Score') for( chr in 2:22 ){ tmp <- fread(paste0('out/preds/EAS_chr',chr,'.profile')) pred[,2] <- pred[,2] + tmp$SCORESUM } # Check predictions are the same as those produced directly by BridgePRS valid <- fread('out/prs-combined_EAS-EUR/EAS_weighted_combined_preds.dat') # Match IDs ptr <- match( valid$IID, pred$IID ) plot( pred$Score[ptr], valid$Weighted, main=\"Individual PRS\" xlab=\"PRS estimated by BridgePRS\", ylab=\"PRS re-estimated from SNP weights\" ) Predictions are the same +/- rounding error and a constant. Back to table of contents","title":"Using BridgePRS SNP weights"},{"location":"practicals/workshop_practical_paul1/","text":"Introduction to Polygenic Scores Analyses 1.1 Key Learning Outcomes After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results 1.2 Resources you will be using To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link Warning All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. 1.3 PRS Definition A Polygenic Risk Score (PRS) is an estimate of an individual\u2019s genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes, weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS. 1.4 Understanding GWAS Summary Statistics When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient ( \\(\\beta\\) ) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \\(\\beta\\) coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \\(\\beta\\) coefficient reflects how much the phenotype changes for each G allele present (NB. The \\(\\beta\\) can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) Information The relationship between the \\(\\beta\\) coefficient from the logistic regression and the OR is: $$ OR = e ^ \\beta $$ and $$ log_{e}(OR) = \\beta $$ While GWAS sometimes convert from the \\(\\beta\\) to the OR when reporting results, most PRS software convert OR back to \\(\\beta\\) 's ( \\(log_e(OR)\\) ) to allow simple addition of \\(log_e(OR)\\) 's. Questions Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. Let us open the Height GWAS file ( day1a/Base_Data/GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). What do these PRS values mean in terms of the height of those individuals? 1.5 Matching the Base and Target Data sets The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. Warning For SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias. 1.6 Linkage Disequilibrium in PRS Analyses GWAS are typically performed one-snp-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent causal genetic effects challenging. There are two main options to address this challenge when calculating PRS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date (Pain et al 2021). In this workshop we will consider option 1 (the \"clumping+thresholding\", or C+T, method), implemented in PRSice (Choi & O'Reilly 2019), but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred (Prive et al 2020), PRS-CS (Ge et al 2019) and SBayesRC (Zheng et al 2024) papers. 1.6.1 Performing Clumping Clumping is the procedure where a SNP data set is `thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \\(P\\) -value. SNPs are first sorted (i.e. ranked) by their \\(P\\) -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \\(r^2\\) > 0.1) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are clumped . This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK (Chang et al 2015). First, you will have to navigate to the website_practical_downloads/day1a folder using the terminal. Next type the following command (NB. See warning below): ./Software/plink_mac \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height Note You can copy & paste code from this document directly to the terminal. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \\(r^2>0.1\\) within a 250 kb window of the index SNPs are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. Questions How many SNPs were in the GIANT_Height.txt file before clumping? How many SNPs remain after clumping? If we change the \\(r^2\\) threshold to 0.2, how many SNPs remain? Why are there now more SNPs remaining? Why is clumping performed for calculation of PRS? (in the standard approach) 1.7 Running PRSice: the \"C+T\" method Now that we know how to perform the clumping part of the \"clumping+thresholding\" (C+T) method, in this section we will learn about the thresholding part, and we will learn how to run PRSice, which performs both parts (C+T) simultaneously. Deciding which SNPs to include is one of the key challenges in the calculation of PRS. A simple and popular approach is to include SNPs according to their GWAS association \\(P\\) -value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \\(P\\) -value thresholds. 1.7.1 Height PRS using GW-significant SNPs only Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the day1a/ directory, run the following command in the terminal: Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --bar-levels 5e-8 \\ --no-full \\ --fastscore \\ --out Results/Height.gws This command takes the Height GWAS summary statistic file ( --base ), informs PRSice of the column name for the column containing the SNP ID ( --snp ), the effect allele ( --A1 ), the non-effect allele ( --A2 ), the effect size ( --stat ) and the \\(P\\) -value ( --pvalue ). We also inform PRSice that the effect size is a \\(\\beta\\) coefficient ( --beta ) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \\(P\\) -value \\(< 5 \\times 10^{-8}\\) ( --bar-levels and --no-full ). Note The default of PRSice is to perform clumping with an \\(r^2\\) threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice type: ./Software/PRSice_mac -h Take some time to look through some of these user options and you should be able to see why the above command produced the desired output. PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \\(\\beta\\) coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \\(\\beta\\) Standard.Error - The standard error of the best-fit PRS \\(\\beta\\) coefficient (see above) P - The \\(P\\) -value relating to testing the null hypothesis that the best-fit PRS \\(\\beta\\) coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \\(P\\) -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. Questions What is the \\(R^2\\) for the PRS constructed using only genome-wide significant SNPs? What is the \\(P\\) -value for the association between the PRS and the outcome? Is this significant? (explain your answer). 1.7.2 Height PRS across multiple P-value thresholds A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among SNPs that did not reach genome-wide significance. However, since we do not know what \\(P\\) -value threshold provides the \"best\" prediction for our particular data, then we can calculate the PRS at several \\(P\\) -value thresholds and test their prediction accuracy to identify the \"best\" threshold (NB. See Dudbridge et al 2013 for theory on factors affecting the best threshold). This process is implemented in PRSice and can be performed automatically as follows: Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --fastscore \\ --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT (Fig.1.1) generated by PRSice. Open up the bar plot image file in the Results/ folder and take a look at the bar plot. Questions Which is the most predictive threshold? What is the \\(R^2\\) of the most predictive threshold and how does it compare to PRS generated using only genome-wide significant SNPs? 1.7.3 High-Resolution Scoring If we only calculate PRS at a small number of \\(P\\) -value thresholds, then we might not observe the most predictive threshold. In PRSice, we can calculate and test PRS at a large number of thresholds using \"high-resolution scoring\" - which we can perform simply by removing the --fastscore command from the PRSice script. Run the following script to perform high-resolution scoring, and check the output files. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --perm 1000 \\ --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot (Fig.1.2) presenting the model fit of PRS calculated at a large number of P -value thresholds. Questions Which is the most predictive threshold? How much better is the threshold identified using high-resolution scoring, in terms of model \\(R^2\\) ? What is the empirical P-value and is it significant? Should we use the raw P-value or the empirical \\(P\\) -value? Why? Which command line option generated the empirical \\(P\\) -value? Information To save time, we will not generate the empirical \\(P\\) -value for most of the rest of this practical. However, when using PRSice for your analyses you should always generate the empirical \\(P\\) -value to check whether the PRS association with the target phenotype is significant when multiple testing (across multiple thresholds) is accounted for. 1.8 Accounting for Covariates When performing PRS analyses, we often want to account for the effect of covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --cov Target_Data/TAR.covariate \\ --cov-col Sex \\ --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This PRS. \\(R^2\\) is calculated from a formula that provides an unbiased estimate of the difference between the \\(R^2\\) of the null model (e.g. \\(Height\\sim Sex\\) ) and the \\(R^2\\) of the full model (e.g. \\(Height\\sim Sex+PRS\\) ). Information Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure. However, it can be tedious to type all 20 or 40 PCs (e.g. PC1,PC2,...,PC20). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substituation of numbers. If @ is found in front of the --cov-col string, then any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1,PC2,PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1,cov3,cov4,cov5 . You can also mix it up E.g. @PC[1-3],Sex will be interpret as PC1,PC2,PC3,Sex by PRSice. Questions How does the inclusion of sex as a covariate change the results? If you want to control for the effect of a categorical variable , such as socio-economic status (SES) or smoking status, then you can use the option --cov-factor in PRSice to automatically generate \"dummy variables\" corresponding to different levels of the factor. The following command shows how this can be done (using sex as an example): Rscript.exe ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --cov Target_Data/TAR.covariate \\ --cov-col Sex \\ --cov-factor Sex \\ --out Results/Height.sex 1.9 Case-Control Studies In the previous exercises, we performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and should be converted to \\(\\beta\\) 's when constructing PRS so that they can be simply added across the genome Here we will use CAD as an example. You will find the summary statistics under Base_Data ( Cardio_CAD.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. Information GWAS summary statistics for binary traits tend to report the OR instead of the \\(\\beta\\) coefficient, in which case the --or should be used. However, CARDIoGRAM plus C4D consortium provided the \\(\\beta\\) coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/Cardio_CAD.txt \\ --target Target_Data/TAR \\ --snp markername \\ --A1 effect_allele \\ --A2 noneffect_allele \\ --chr chr \\ --bp bp_hg19 \\ --stat beta \\ --beta \\ --pvalue p_dgc \\ --binary-target T \\ --pheno Target_Data/TAR.cad \\ --perm 100 \\ --out Results/CAD.highres Information If --chr and --bp are added to the command line (as above) then PRSice can check whether the SNPs in the Base and Target data have the same chromosomal coordinates. Questions What is the \\(R^2\\) and \\(P\\) -value of the best-fit PRS? Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample? 1.10 Generating PRS Quantile Plots When considering the potential clinical utility of PRS, researchers often want to see the risk of disease among individuals in e.g. the top 5% or top 1% of PRS values compared to the rest of the population. Quantile plots are a nice way to illustrate the potential clinical utility of PRS (Fig.1.3) - made popular in the landmark PRS paper on the effects of PRS on cardiovascular disease by Khera et al (2018) - but should also be interpreted with some caution (see Fig.5 in: Choi, Mak & O'Reilly 2020). To generate quantile plots in PRSice, simply add the --quantile option and specify the number of quantiles that you would like to plot (e.g. --quantile 10 for deciles). Note If you have already calculated the PRS using a previous command, then we can skip repeating the same calculation by using the --plot option Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/Cardio_CAD.txt \\ --target Target_Data/TAR \\ --snp markername \\ --A1 effect_allele \\ --A2 noneffect_allele \\ --chr chr \\ --bp bp_hg19 \\ --stat beta \\ --beta \\ --pvalue p_dgc \\ --binary-target T \\ --pheno Target_Data/TAR.cad \\ --perm 100 \\ --quantile 10 \\ --plot \\ --out Results/CAD.highres Information The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is useful when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles - and again with 50 quantiles - checking the quantile plot each time. Questions What is the approximate higher risk of CAD for individuals in the top 5% of CAD PRS compared to those individuals with an average CAD PRS? Quantile plots only separate samples into quantiles of equal size. However, we can also use PRSice to produce a more flexible \"strata plot\", where quantiles of different sizes can be displayed in the same plot (e.g. Fig.1.3). We use --quant-break , corresponding to the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile (best for interpretation of results if this is the central quantile): Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/Cardio_CAD.txt \\ --target Target_Data/TAR \\ --snp markername \\ --A1 effect_allele \\ --A2 noneffect_allele \\ --chr chr \\ --bp bp_hg19 \\ --stat beta \\ --beta \\ --pvalue p_dgc \\ --binary-target T \\ --pheno Target_Data/TAR.cad \\ --perm 100 \\ --quantile 100 \\ --quant-break 1,5,10,20,40,60,80,90,95,99,100 \\ --quant-ref 60 \\ --plot \\ --out Results/CAD.highres Note You can see these results as plots in the files *_QUANTILES_PLOT_* or *_STRATA_PLOT_*, while the quantile results can also be seen in Table form in the *_QUANTILES_* file. The results shown in the quantile and strata plots here do not show much higher risk in the top quantiles, but this is because our simulated target data are small. 1.11 Cross-Trait Analysis A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by Ruderfer et al 2014 (Fig.1.5), which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.cad \\ --binary-target T \\ --out Results/Cross.highres Questions What is the \\(R^2\\) for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Now try using CAD as the base to predict height as the target trait? What is the PRS \\(R^2\\) for that? Can you explain why the results might be different when the phenotypes have been switched between the base and target? Or do you think the results should be the same? Tip To perform a cross-trait analysis involving a large number of target traits, you can use the --pheno-col option, which allows multiple target phenotypes to be included in the analysis simultaneously.","title":"Day1 (AM)"},{"location":"practicals/workshop_practical_paul1/#introduction-to-polygenic-scores-analyses","text":"","title":"Introduction to Polygenic Scores Analyses"},{"location":"practicals/workshop_practical_paul1/#11-key-learning-outcomes","text":"After completing this practical, you should be able to: Perform basic Polygenic Risk Score (PRS) analyses using PRSice: ( Euesden, Lewis & O'Reilly 2015 ; Choi & O'Reilly 2019 ) Interpret results generated from PRS analyses Customise visualisation of results","title":"1.1 Key Learning Outcomes"},{"location":"practicals/workshop_practical_paul1/#12-resources-you-will-be-using","text":"To perform PRS analyses, summary statistics from Genome-Wide Association Studies (GWAS) are required. In this workshop, the following summary statistics are used: Phenotype Provider Description Download Link Height GIANT Consortium GWAS of height on 253,288 individuals Link Coronary artery disease (CAD) CARDIoGRAMplusC4D Consortium GWAS on 60,801 CAD cases and 123,504 controls Link Warning All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only.","title":"1.2 Resources you will be using"},{"location":"practicals/workshop_practical_paul1/#13-prs-definition","text":"A Polygenic Risk Score (PRS) is an estimate of an individual\u2019s genetic propensity to a phenotype, calculated as a sum of their genome-wide genotypes, weighted by corresponding genotype effect sizes obtained from GWAS summary statistics. In the next section we will consider what the effect size means and how it is used in computing PRS.","title":"1.3 PRS Definition"},{"location":"practicals/workshop_practical_paul1/#14-understanding-gwas-summary-statistics","text":"When GWAS are performed on a quantitative trait, the effect size is typically given as a beta coefficient ( \\(\\beta\\) ) from a linear regression with Single Nucleotide Polymorphism (SNP) genotypes as predictor of phenotype. The \\(\\beta\\) coefficient estimates the increase in the phenotype for each copy of the effect allele . For example, if the effect allele of a SNP is G and the non-effect allele is A , then the genotypes AA , AG and GG will be coded as 0, 1 and 2 respectively. In this scenario, the \\(\\beta\\) coefficient reflects how much the phenotype changes for each G allele present (NB. The \\(\\beta\\) can be positive or negative - so the 'effect allele' is simply the allele that was coded in the regression, not necessarily the allele with a positive effect). When a GWAS is performed on a binary trait (e.g. case-control study), the effect size is usually reported as an Odd Ratios (OR). Using the same example, if the OR from the GWAS is 2 with respect to the G allele, then the OR of AG relative to AA is 2, and the OR of GG relative to AA is 4. So an individual with the GG genotype are estimated* to be 4 times more likely to be a case than someone with the AA genotype (*an Odds Ratio is itself an estimate of a Risk Ratio, which cannot be calculated from a case/control study) Information The relationship between the \\(\\beta\\) coefficient from the logistic regression and the OR is: $$ OR = e ^ \\beta $$ and $$ log_{e}(OR) = \\beta $$ While GWAS sometimes convert from the \\(\\beta\\) to the OR when reporting results, most PRS software convert OR back to \\(\\beta\\) 's ( \\(log_e(OR)\\) ) to allow simple addition of \\(log_e(OR)\\) 's. Questions Column names are not standardised across reported GWAS results, thus it is important to check which column is the effect (coded) allele and which is the non-effect allele. For example, in the height GWAS conducted by the GIANT consortium, the effect allele is in the column Allele1, while Allele2 represents the non-effect allele. Let us open the Height GWAS file ( day1a/Base_Data/GIANT_Height.txt ) and inspect the SNPs at the top of the file. If we only consider SNPs rs4747841 and rs878177 , what will the \u2018PRS\u2019 of an individual with genotypes AA and TC , respectively, be? And what about for an individual with AG and CC , respectively? (Careful these are not easy to get correct! This shows how careful PRS algorithms/code need to be). What do these PRS values mean in terms of the height of those individuals?","title":"1.4 Understanding GWAS Summary Statistics"},{"location":"practicals/workshop_practical_paul1/#15-matching-the-base-and-target-data-sets","text":"The first step in PRS calculation is to ensure consistency between the GWAS summary statistic file ( base data ) and the target genotype file ( target data ). Since the base and target data are generated independently, they often relate to different SNPs and so the first job is to identify the overlapping SNPs across the two data sets and remove non-overlapping SNPs (this is usually done for you by PRS software). If the overlap is low then it would be a good idea to perform imputation on your target data to increase the number of SNPs that overlap between the data sets. The next, more tricky issue, is that the genotype encoding between the data sets may differ. For example, while the effect allele of a SNP is T in the base data, the effect allele in the target might be G instead. When this occurs, allele flipping should be performed,where the genotype encoding in the target data is reversed so that TT , TG and GG are coded as 2, 1 and 0. Again, this is usually performed automatically by PRS software. Warning For SNPs that have complementary alleles, e.g. A/T , G/C , we cannot be certain that the alleles referred to in the target data correspond to those of the base data or whether they are the 'other way around' due to being on the other DNA strand (unless the same genotyping chip was used for all data). These SNPs are known as ambiguous SNPs , and while allele frequency information can be used to match the alleles, we remove ambiguous SNPs in PRSice to avoid the possibility of introducting unknown bias.","title":"1.5 Matching the Base and Target Data sets"},{"location":"practicals/workshop_practical_paul1/#16-linkage-disequilibrium-in-prs-analyses","text":"GWAS are typically performed one-snp-at-a-time, which, combined with the strong correlation structure across the genome (Linkage Disequilibrium (LD)), makes identifying the independent causal genetic effects challenging. There are two main options to address this challenge when calculating PRS: 1. SNPs are clumped so that the retained SNPs are largely independent of each other, allowing their effects to be summed, assuming additive effects, 2. all SNPs are included and the LD between them is accounted for. While option 2 is statistically appealing, option 1 has been most adopted in PRS studies so far, most likely due to its simplicity and the similarity of results of methods using the different options to date (Pain et al 2021). In this workshop we will consider option 1 (the \"clumping+thresholding\", or C+T, method), implemented in PRSice (Choi & O'Reilly 2019), but if you are interested in how LD can be incorporated as a parameter in PRS calculation then see the LDpred (Prive et al 2020), PRS-CS (Ge et al 2019) and SBayesRC (Zheng et al 2024) papers.","title":"1.6 Linkage Disequilibrium in PRS Analyses"},{"location":"practicals/workshop_practical_paul1/#161-performing-clumping","text":"Clumping is the procedure where a SNP data set is `thinned' by removing SNPs across the genome that are correlated (in high LD) with a nearby SNP that has a smaller association \\(P\\) -value. SNPs are first sorted (i.e. ranked) by their \\(P\\) -values. Then, starting from the most significant SNP (denoted as the index SNP ), any SNPs in high LD (eg. \\(r^2\\) > 0.1) with the index SNP are removed. To reduce computational burden, only SNPs that are within e.g. 250 kb of the index SNP are clumped . This process is continued until no index SNPs remain. Use the command below to perform clumping of the Height GWAS data using PLINK (Chang et al 2015). First, you will have to navigate to the website_practical_downloads/day1a folder using the terminal. Next type the following command (NB. See warning below): ./Software/plink_mac \\ --bfile Target_Data/TAR \\ --clump Base_Data/GIANT_Height.txt \\ --clump-p1 1 \\ --clump-snp-field MarkerName \\ --clump-field p \\ --clump-kb 250 \\ --clump-r2 0.1 \\ --out Results/Height Note You can copy & paste code from this document directly to the terminal. The command above performs clumping on the height GWAS using LD calculated based on the TAR genotype file. SNPs that have \\(r^2>0.1\\) within a 250 kb window of the index SNPs are removed. This will generate the Height.clumped file, which contains the SNPs retained after clumping. Questions How many SNPs were in the GIANT_Height.txt file before clumping? How many SNPs remain after clumping? If we change the \\(r^2\\) threshold to 0.2, how many SNPs remain? Why are there now more SNPs remaining? Why is clumping performed for calculation of PRS? (in the standard approach)","title":"1.6.1 Performing Clumping"},{"location":"practicals/workshop_practical_paul1/#17-running-prsice-the-ct-method","text":"Now that we know how to perform the clumping part of the \"clumping+thresholding\" (C+T) method, in this section we will learn about the thresholding part, and we will learn how to run PRSice, which performs both parts (C+T) simultaneously. Deciding which SNPs to include is one of the key challenges in the calculation of PRS. A simple and popular approach is to include SNPs according to their GWAS association \\(P\\) -value. For example, we may choose to include only the genome-wide significant SNPs from the GWAS because those are the SNPs with significant evidence for association. In the next subsection you will compute PRS from GW-significant SNPs only, and then in the subsequent subsection you will generate multiple PRSs using different \\(P\\) -value thresholds.","title":"1.7 Running PRSice: the \"C+T\" method"},{"location":"practicals/workshop_practical_paul1/#171-height-prs-using-gw-significant-snps-only","text":"Use the commands below to run PRSice with GIANT Height GWAS as base data and the height phenotype target data. PRSice will calculate Height PRS in the target data and then perform a regression of the Height PRS against the target individual's true height values. From the day1a/ directory, run the following command in the terminal: Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --bar-levels 5e-8 \\ --no-full \\ --fastscore \\ --out Results/Height.gws This command takes the Height GWAS summary statistic file ( --base ), informs PRSice of the column name for the column containing the SNP ID ( --snp ), the effect allele ( --A1 ), the non-effect allele ( --A2 ), the effect size ( --stat ) and the \\(P\\) -value ( --pvalue ). We also inform PRSice that the effect size is a \\(\\beta\\) coefficient ( --beta ) instead of an OR. The --binary-target F command informs PRSice that the target phenotype is a quantitative trait and thus linear regression should be performed. In addition, we ask PRSice not to perform high-resolution scoring over multiple thresholds (--fastscore), and to compute the PRS using only those SNPs with \\(P\\) -value \\(< 5 \\times 10^{-8}\\) ( --bar-levels and --no-full ). Note The default of PRSice is to perform clumping with an \\(r^2\\) threshold of 0.1 and a window size of 250kb. To see a full list of command line options available in PRSice type: ./Software/PRSice_mac -h Take some time to look through some of these user options and you should be able to see why the above command produced the desired output. PRSice performs strand flipping and clumping automatically and generates the Height.gws.summary file, together with other output that we will look into later in the practical. The summary file contains the following columns: Phenotype - Name of Phenotype Set - Name of Gene Set. Default is Base Threshold - Best P-value Threshold PRS.R2 - Variance explained by the PRS Full.R2 - Variance explained by the full model (including the covariates) Null.R2 - Variance explained by the covariates (none provided here) Prevalence - The population disease prevalence as indicated by the user (not provided here due to testing continuous trait) Coefficient - The \\(\\beta\\) coefficient corresponding to the effect estimate of the best-fit PRS on the target trait in the regression. A one unit increase in the PRS increases the outcome by \\(\\beta\\) Standard.Error - The standard error of the best-fit PRS \\(\\beta\\) coefficient (see above) P - The \\(P\\) -value relating to testing the null hypothesis that the best-fit PRS \\(\\beta\\) coefficient is zero. Num_SNP - Number of SNPs included in the best-fit PRS Empirical-P - Only provided if permutation is performed. This is the empirical \\(P\\) -value corresponding to the association test of the best-fit PRS - this controls for the over-fitting that occurs when multiple thresholds are tested. For now, we can ignore most columns and focus on the PRS.R2 and the P column, which provide information on the model fit. Questions What is the \\(R^2\\) for the PRS constructed using only genome-wide significant SNPs? What is the \\(P\\) -value for the association between the PRS and the outcome? Is this significant? (explain your answer).","title":"1.7.1 Height PRS using GW-significant SNPs only"},{"location":"practicals/workshop_practical_paul1/#172-height-prs-across-multiple-p-value-thresholds","text":"A disadvantage of using only genome-wide significant SNPs is that there are likely to be many true signals among SNPs that did not reach genome-wide significance. However, since we do not know what \\(P\\) -value threshold provides the \"best\" prediction for our particular data, then we can calculate the PRS at several \\(P\\) -value thresholds and test their prediction accuracy to identify the \"best\" threshold (NB. See Dudbridge et al 2013 for theory on factors affecting the best threshold). This process is implemented in PRSice and can be performed automatically as follows: Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --fastscore \\ --out Results/Height.fast By removing the --bar-levels and --no-full command, we ask PRSice to perform PRS calculation with a number of predefined thresholds (0.001, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1). The .prsice file is very similar to the .summary file, the only difference is that .prsice file reports the results of model fits for all thresholds instead of the most predictive threshold. This allow us to observe the change in model fitting across different thresholds, visualized by the BARPLOT (Fig.1.1) generated by PRSice. Open up the bar plot image file in the Results/ folder and take a look at the bar plot. Questions Which is the most predictive threshold? What is the \\(R^2\\) of the most predictive threshold and how does it compare to PRS generated using only genome-wide significant SNPs?","title":"1.7.2 Height PRS across multiple P-value thresholds"},{"location":"practicals/workshop_practical_paul1/#173-high-resolution-scoring","text":"If we only calculate PRS at a small number of \\(P\\) -value thresholds, then we might not observe the most predictive threshold. In PRSice, we can calculate and test PRS at a large number of thresholds using \"high-resolution scoring\" - which we can perform simply by removing the --fastscore command from the PRSice script. Run the following script to perform high-resolution scoring, and check the output files. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --perm 1000 \\ --out Results/Height.highres When PRSice performs high-resolution scoring, it will generate a plot (Fig.1.2) presenting the model fit of PRS calculated at a large number of P -value thresholds. Questions Which is the most predictive threshold? How much better is the threshold identified using high-resolution scoring, in terms of model \\(R^2\\) ? What is the empirical P-value and is it significant? Should we use the raw P-value or the empirical \\(P\\) -value? Why? Which command line option generated the empirical \\(P\\) -value? Information To save time, we will not generate the empirical \\(P\\) -value for most of the rest of this practical. However, when using PRSice for your analyses you should always generate the empirical \\(P\\) -value to check whether the PRS association with the target phenotype is significant when multiple testing (across multiple thresholds) is accounted for.","title":"1.7.3 High-Resolution Scoring"},{"location":"practicals/workshop_practical_paul1/#18-accounting-for-covariates","text":"When performing PRS analyses, we often want to account for the effect of covariates. Based on user inputs, PRSice can automatically incorporate covariates into its model. For example, the following commands will include sex into the regression model as a covariate: Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --cov Target_Data/TAR.covariate \\ --cov-col Sex \\ --out Results/Height.sex When covariates are included in the analysis, PRSice will use the model fit of only PRS for all its output. This PRS. \\(R^2\\) is calculated from a formula that provides an unbiased estimate of the difference between the \\(R^2\\) of the null model (e.g. \\(Height\\sim Sex\\) ) and the \\(R^2\\) of the full model (e.g. \\(Height\\sim Sex+PRS\\) ). Information Usually, Principal Components (PCs) are also included as covariates in the analysis to account for population structure. However, it can be tedious to type all 20 or 40 PCs (e.g. PC1,PC2,...,PC20). In PRSice, you can add @ in front of the --cov-col string to activate the automatic substituation of numbers. If @ is found in front of the --cov-col string, then any numbers within [ and ] will be parsed. E.g. @PC[1-3] will be read as PC1,PC2,PC3 . Discontinuous input are also supported: @cov[1.3-5] will be parsed as cov1,cov3,cov4,cov5 . You can also mix it up E.g. @PC[1-3],Sex will be interpret as PC1,PC2,PC3,Sex by PRSice. Questions How does the inclusion of sex as a covariate change the results? If you want to control for the effect of a categorical variable , such as socio-economic status (SES) or smoking status, then you can use the option --cov-factor in PRSice to automatically generate \"dummy variables\" corresponding to different levels of the factor. The following command shows how this can be done (using sex as an example): Rscript.exe ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --cov Target_Data/TAR.covariate \\ --cov-col Sex \\ --cov-factor Sex \\ --out Results/Height.sex","title":"1.8 Accounting for Covariates"},{"location":"practicals/workshop_practical_paul1/#19-case-control-studies","text":"In the previous exercises, we performed PRS analyses on height, which is a quantitative trait. For binary phenotypes (e.g case-control) there are a number of differences in the analysis: Logistic regression has to be performed instead of linear regression ORs are usually provided and should be converted to \\(\\beta\\) 's when constructing PRS so that they can be simply added across the genome Here we will use CAD as an example. You will find the summary statistics under Base_Data ( Cardio_CAD.txt ) and the phenotype file ( TAR.cad ) under Target_Data . You will also need to specify --binary-target T in the PRSice command to indicate that the phenotype is binary. Information GWAS summary statistics for binary traits tend to report the OR instead of the \\(\\beta\\) coefficient, in which case the --or should be used. However, CARDIoGRAM plus C4D consortium provided the \\(\\beta\\) coefficient, therefore we will include --beta in our code and specify --binary-target T to indicate that the phenotype is binary. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/Cardio_CAD.txt \\ --target Target_Data/TAR \\ --snp markername \\ --A1 effect_allele \\ --A2 noneffect_allele \\ --chr chr \\ --bp bp_hg19 \\ --stat beta \\ --beta \\ --pvalue p_dgc \\ --binary-target T \\ --pheno Target_Data/TAR.cad \\ --perm 100 \\ --out Results/CAD.highres Information If --chr and --bp are added to the command line (as above) then PRSice can check whether the SNPs in the Base and Target data have the same chromosomal coordinates. Questions What is the \\(R^2\\) and \\(P\\) -value of the best-fit PRS? Does this suggest that there is a significant association between the CAD PRS and CAD status in the target sample?","title":"1.9 Case-Control Studies"},{"location":"practicals/workshop_practical_paul1/#110-generating-prs-quantile-plots","text":"When considering the potential clinical utility of PRS, researchers often want to see the risk of disease among individuals in e.g. the top 5% or top 1% of PRS values compared to the rest of the population. Quantile plots are a nice way to illustrate the potential clinical utility of PRS (Fig.1.3) - made popular in the landmark PRS paper on the effects of PRS on cardiovascular disease by Khera et al (2018) - but should also be interpreted with some caution (see Fig.5 in: Choi, Mak & O'Reilly 2020). To generate quantile plots in PRSice, simply add the --quantile option and specify the number of quantiles that you would like to plot (e.g. --quantile 10 for deciles). Note If you have already calculated the PRS using a previous command, then we can skip repeating the same calculation by using the --plot option Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/Cardio_CAD.txt \\ --target Target_Data/TAR \\ --snp markername \\ --A1 effect_allele \\ --A2 noneffect_allele \\ --chr chr \\ --bp bp_hg19 \\ --stat beta \\ --beta \\ --pvalue p_dgc \\ --binary-target T \\ --pheno Target_Data/TAR.cad \\ --perm 100 \\ --quantile 10 \\ --plot \\ --out Results/CAD.highres Information The --plot option tells PRSice to generate the plots without re-running the whole PRSice analysis. This is useful when you want to change some of the parameters for plotting e.g. the number of quantiles. Try running the previous command with 20 quantiles - and again with 50 quantiles - checking the quantile plot each time. Questions What is the approximate higher risk of CAD for individuals in the top 5% of CAD PRS compared to those individuals with an average CAD PRS? Quantile plots only separate samples into quantiles of equal size. However, we can also use PRSice to produce a more flexible \"strata plot\", where quantiles of different sizes can be displayed in the same plot (e.g. Fig.1.3). We use --quant-break , corresponding to the upper bound of each strata, and --quant-ref , which represents the upper bound of the reference quantile (best for interpretation of results if this is the central quantile): Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/Cardio_CAD.txt \\ --target Target_Data/TAR \\ --snp markername \\ --A1 effect_allele \\ --A2 noneffect_allele \\ --chr chr \\ --bp bp_hg19 \\ --stat beta \\ --beta \\ --pvalue p_dgc \\ --binary-target T \\ --pheno Target_Data/TAR.cad \\ --perm 100 \\ --quantile 100 \\ --quant-break 1,5,10,20,40,60,80,90,95,99,100 \\ --quant-ref 60 \\ --plot \\ --out Results/CAD.highres Note You can see these results as plots in the files *_QUANTILES_PLOT_* or *_STRATA_PLOT_*, while the quantile results can also be seen in Table form in the *_QUANTILES_* file. The results shown in the quantile and strata plots here do not show much higher risk in the top quantiles, but this is because our simulated target data are small.","title":"1.10 Generating PRS Quantile Plots"},{"location":"practicals/workshop_practical_paul1/#111-cross-trait-analysis","text":"A popular application of PRS is in performing cross-trait analyses. This allows some interesting analyses such as those performed by Ruderfer et al 2014 (Fig.1.5), which used the bipolar PRS to predict into different clinical dimensions of schizophrenia. In this practical, we will perform cross-trait analyses between CAD and Height, using height as the base and CAD as the target. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.cad \\ --binary-target T \\ --out Results/Cross.highres Questions What is the \\(R^2\\) for the most predictive threshold when using height as the base phenotype and CAD as the target phenotype? Now try using CAD as the base to predict height as the target trait? What is the PRS \\(R^2\\) for that? Can you explain why the results might be different when the phenotypes have been switched between the base and target? Or do you think the results should be the same? Tip To perform a cross-trait analysis involving a large number of target traits, you can use the --pheno-col option, which allows multiple target phenotypes to be included in the analysis simultaneously.","title":"1.11 Cross-Trait Analysis"},{"location":"practicals/workshop_practical_paul2/","text":"Day 2 - Afternoon practical: Pathway PRS analyses Key Learning Outcomes After completing this practical, you should be able to: Understand the motivation and rationale for calculating gene set PRS. Understand the inputs required for gene set PRS analysis using PRSet. Calculate gene set based PRSs using PRSet. Understand and interpret the outcomes of gene set PRSs and how they differ from genome-wide PRS. Data Structure You will find all practical materials in the following link . Relevant materials that you should find at the start of the practical are: \ud83d\udcc2: Base_Data GIANT_Height.txt \ud83d\udcc2: Target_Data TAR.fam TAR.bim TAR.bed TAR.height TAR.covariate \ud83d\udcc1: Reference Homo_sapiens.GRCh38.109.gtf.gz Sets.gmt \ud83d\udcc1: Software PRSice.R PRSice_mac/linux Introduction to gene set (pathway) PRS analysis Most PRS methods summarize genetic risk to a single number, based on the aggregation of an individual\u2019s genome-wide risk alleles. This approach does not consider the different contributions of the various biological processes that can influence complex diseases and traits. During this session, you will learn how to run a gene set (or pathway) based PRS analyses. The key difference between genome-wide PRS and gene set or pathway-based PRSs analyses is that, instead of aggregating the estimated effects of risk alleles across the entire genome, gene set PRSs aggregate risk alleles across as many gene sets as the user defines (Figure 1). Figure 1: The pathway polygenic risk score approach. Coloured boxes represent genes, lines link genes that are within the same genomic pathway. See full description here . Note In this practical, we will go through some of the input requirements and considerations for the analysis of gene set PRS analysis, and will then calculate some gene set based PRS using PRSet . By aggregating PRS across multiple gene sets (or pathways), these PRS analyses will allow us to determine the genetic contribution made by each biological process in complex traits and diseases. For more information about the rationale and the software that we are going to use, please see the PRSet publication PRSet: Pathway-based polygenic risk score analyses and software . Questions \u2753 Why is it useful to have polygenic scores measured across gene-sets (or pathways) for individuals? \u2753 Isn\u2019t it su\ufb03cient to just obtain a ranking of gene-sets according to GWAS-signal enrichment (using gene set enrichment tools such as MAGMA or partitioned LDSC)? Inputs required for gene-set PRS analysis Summary statistics from GWAS, as well as individual level genotype and phenotype data are required to perform gene set PRS analyses. In this session, the following Base and Target data is used. Base data is publicly available. All Target data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Additionally, to perform gene set PRS analyses, information about the gene sets for which we want to calculate the PRSs are required. In this tutorial, we will use as input gene-sets from the Molecular Signatures Database (in Reference/ ). Note that PRSet also takes as input BED and SNP files . Data Set Description Download Link Ensembl Human Genome GTF file A file containing the coordinates for genes in the human genome. Used by PRSet to map the SNPs onto genic regions Link to Homo_sapiens.GRCh38.109.gtf.gz MSigDB Gene Sets File containing the gene-set information. Free registration required. Download link after registration Molecular Signatures Database MSigDB + General Transfer Format file MSigDB o\ufb00ers an excellent source of gene sets, including the hallmark genes, gene sets of di\ufb00erent biological processes, gene sets of di\ufb00erent oncogenic signatures etc. All gene sets from MSigDB follows the Gene Matrix Transposed file format (GMT), which consists of one line per gene set, each containing at least 3 column of data: Set A Description Gene 1 Gene 2 ... Set B Description Gene 1 Gene 2 ... Tips While you can read the GMT file using Excel, we recommend exploring these files using bash. You should be aware that Excel has a tendency to convert gene names into dates (e.g. SEPT9 to Sep-9) Have a look at the Reference/Sets.gmt file. Questions \u2753 How many gene sets are there in the Reference/Sets.gmt file? \u2753 How many genes does the largest gene set contain? Hint: You can use the command awk '{print $1\"\\t\"NF}' Sets.gmt As GMT format does not contain the chromosomal location for each individual gene, an additional file (General Transfer Format file) is required to provide the chromosomal location such that SNPs can be mapped to genes. The General Transfer Format (GTF) file contains the chromosomal coordinates for each gene. It is a tab separated file and all fields except the last must contain a value. You can read the full format specification here . Two columns in the GTF file that might be of particular interest are: - Column 3: feature , which indicates what feature that line of GTF represents. This allows us to select or ignore features that are of interest. Column 9: attribute , which contains a semicolon-separated list of tag-value pairs (separated by a space), providing additional information about each feature. A key can be repeated multiple times. Other inputs that can be used for gene set PRS using PRSet Browser Extensible Data BED Browser Extensible Data (BED) file (di\ufb00erent to the binary ped file from PLINK), is a file format to define genetic regions. It contains 3 required fields per line (chromosome, start coordinate and end coordinate) together with 9 additional optional field. A special property of BED is that it is a 0-based format, i.e. chromosome starts at 0, as opposed to the usual 1-based format such as the PLINK format. For example, a SNP on chr1:10000 will be represented as: 1 9999 10000 Questions \u2753 How should we represent the coordinate of rs2980300 (chr1:785989) in BED format? List of SNPs Finally, PRSet also allow SNP sets, where the user have flexibility to decide what SNPs are included. The list of SNPs can have two different formats: - SNP list format, a file containing a single column of SNP ID. Name of the set will be the file name or can be provided using --snp-set File:Name - MSigDB format: Each row represent a single SNP set with the first column containing the name of the SNP set. Exercise: Calculate gene set PRS analysis To perform the PRSet analysis, we need to provide the GTF file and the GMT file to PRSice. Additionally, we want to specify the number of permutation to calculate competitive P-value calculation using the --set-perm option. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --A1 Allele1 \\ --A2 Allele2 \\ --snp MarkerName \\ --pvalue p \\ --stat b \\ --beta \\ --binary-target F \\ --pheno Target_Data/TAR.height \\ --cov Target_Data/TAR.covariate \\ --out Height.set \\ --gtf Reference/Homo_sapiens.GRCh38.109.gtf.gz \\ --wind-5 5kb \\ --wind-3 1kb \\ --msigdb Reference/Sets.gmt \\ --multi-plot 10 \\ --set-perm 1000 Compare the code to calculate a genome-wide PRS with PRSice (e.g. looking at yesterday's practical) and the code above to run PRSet Questions \u2753 What extra commands are used today to run gene set specific PRSs? What do these additional commands do? Results and Plots specific of gene set PRS analyses Competitive P-value calculation When only one region of the genome is used to calculate PRSs (for example a gene set or a pathway PRSs), self-contained and/or competitive tests of association can be performed. The null-hypothesis of self-contained and competitive test statistics is di\ufb00erent: - Null-hypothesis for self-contained test - None of the genes within the gene set are associated with the phenotype. - Null-hypothesis for competitive tests - Genes within the gene set are no more associated with the phenotype than genes outside the gene set. Importantly, in a self-contained test, a bigger gene-set will have a higher likelihood of having a significant P-value from self-contained test, which is not desirable. Therefore, competitive P-values should be calculated to account for gene set size, as shown in Figure 2 . Figure 2 : Examples of significant and non-significant gene sets when running competitive tests of association. Check the .summary results file after running PRSice WITH and WITHOUT including the PRSet specific options Questions \u2753 How does the .summary output file change? What extra information (i.e. extra columns) are incorporated when including PRSet specific commands? \u2753 What are the 3 gene-sets with the smallest self contained P-values? \u2753 What are the 3 gene-sets with the smallest competitive P-values? Imagine that you are running an analysis to find the gene sets most associated with height Questions \u2753 Considering both the competitive P-value results, what gene set do you think is the most interesting and why? In addition to additional information in the output files, running the PRSet options will provide one extra figure with the results for the gene set PRS with the highest R2. Figure 3 : An example of the multi-set plot. Sets are sorted based on their self-contained R2. Base is the genome wide PRS. Other considerations when analysing and interpreting gene set PRSs Clumping for each gene set independently In standard clumping and P-value thresholding methods, clumping is performed to account for linkage disequilibrium between SNPs. However, when performing set based analysis, special care are required to perform clumping. Take the following as an example: Assume that: Light Blue fragments are the intergenic regions Dark Blue fragments are the genic regions Red fragments are the gene set regions SNPs are represented as thunder bolt, with the \"index\" SNP in clumping denoted by the green thunderbolt If we simply perform a genome wide clumping, we might remove all SNPs residing within the gene set of interest, reducing the signal: Therefore, to maximize signal within each gene set, we must perform clumping for each gene sets separately: Check the .summary results file (Do not count the Base, as this result corresponds to the genome-wide PRS) Questions \u2753 Can you plot the relationship between the gene set R2 and the number of SNPs in each gene set? What general trend can be seen? P-value thresholding in gene set PRS analyses PRSet default option is to no not perform p-value thresholding. It will simply calculate the set based PRS at P-value threshold of 1. Questions \u2753 Why do you think that the default option of PRSet is P-value threshold of 1? \u2753 In what cases would you like to apply P-value thresholding? Advanced Polygenic Risk Score Analyses Key Learning Outcomes After completing this practical, you should be able to: Know how to adjust for ascertainment bias in case-control analysis Know how over-fitting a\ufb00ects PRS results and how to handle it Understand distribution of PRS Data Structure Relevant materials that you should find at the start of the practical are: \ud83d\udcc2: Base_Data GIANT_Height.txt Cardio_CAD.txt \ud83d\udcc2: Target_Data TAR.fam TAR.bim TAR.bed TAR.height TAR.cad TAR.covariate VAL.fam VAL.bim VAL.bed VAL.height VAL.covariate \ud83d\udcc1: Software PRSice.R PRSice_mac/linux Quantile.R Warning Note All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Exercise 1: Estimating \\(R^2\\) in case and control studies Bias in \\(R^2\\) estimation caused by ascertained case/control samples can be adjusted using the equation proposed by Lee et al (2011) , which requires the sample prevalence (case/control ratio) and population prevalence as parameters. This function is implemented in PRSice and the adjustment can be performed by providing the population prevalence to the command --prevalence . Residuals of logistic regression is not well defined, and in PRS analyses, Nagelkerke \\(R^2\\) is usually used to represent the model \\(R^2\\) (this is the default of PRSice). However, this \\(R^2\\) does not account for the di\ufb00erence between sample prevalence (i.e. case-control ratio) and population prevalence, which can lead to bias in the reported \\(R^2\\) (Figure 1.1a). Figure 1.1: Performance of di\ufb00erent \\(R^2\\) when the study contains equal portion of cases and controls (a) Nagelkerke \\(R^2\\) Bias in \\(R^2\\) estimation caused by ascertained case/control samples can be adjusted using the equation proposed by Lee et al. 2011 (Figure 1.1b) , which requires the sample prevalence (case/control ratio) and population prevalence as parameters. This function is implemented in PRSice and the adjustment can be performed by providing the population prevalence to the command --prevalence . Figure 1.1: Performance of di\ufb00erent \\(R^2\\) when the study contains equal portion of cases and controls **(b) Lee adjusted \\(R^2\\) ** Now, account for the ascertainment of the case/control sample by including the population prevalence (let\u2019s assume e.g. 5% here) in the PRSice command to obtain the adjusted (Lee) \\(R^2\\) : Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/Cardio_CAD.txt \\ --target Target_Data/TAR \\ --snp markername \\ --A1 effect_allele \\ --A2 noneffect_allele \\ --chr chr \\ --bp bp_hg19 \\ --stat beta \\ --beta \\ --pvalue p_dgc \\ --pheno Target_Data/TAR.cad \\ --prevalence 0.05 \\ --binary-target T \\ --out Results/CAD.highres.LEER2 The results are written to the \"Results\" directory. Examine the results folder and each file that was generated. For more information about each file type, see here . Note Check the *.summary file in the Results folder where you will find the usual (Nagelkerke) \\(R^2\\) and the adjusted (Lee) \\(R^2\\) . Figure 1.2: Barplot of CAD Lee \\(R^2\\) Tip To speed up the practical, we have generated a smaller gene-set file. If you want the full gene-set file, you can download it from the link above. All target phenotype data in this workshop are simulated. While they reflect the corresponding trait data, they have no specific biological meaning and are for demonstration purposes only. Questions \u2753 Has accounting for the population prevalence a\ufb00ected the \\(R^2\\) ? \u2753 Would you expect a di\ufb00erence between the Nagelkerke \\(R^2\\) and the Lee adjusted \\(R^2\\) if the case/control ratio in the target sample reflects the disease prevalence in the population? Exercise 2: Overfitting caused by model optimisation In PRS analyses, the shrinkage or tuning parameter is usually optimized across a wide range of parametric space (e.g. P -value threshold, proportion of causal SNPs). When both optimisation and association testing are performed on the target data, over-fitted results will be obtained. The accuracy and predictive power of over-fitted results are likely to diminish when replicated in an independent data set. A simple solution is to perform permutation to obtain an empirical P -value for the association model, which is implemented in PRSice. Briefly, permutation is performed as follows: Compute the P -value in your original data, denoted as obs.p, at the \"best\" threshold. Then shu\ufb04e the phenotype and obtain the P -value of the \"best\" threshold for this null phenotype, denoted as null.p Repeat 2 \\(N\\) times Calculate the empirical P-value ( \\(P_{emp}\\) ) as: $$ P_{emp} = \\frac{\\sum_{n=1}^NI(P_{null}\\lt P_observed)+1}{N+1} $$ where \\(I(.)\\) is the indicator function. You will have to specify the number of permutation (N) to perform by providing --perm N as a parameter to PRSice. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --cov Target_Data/TAR.covariate \\ --cov-col Sex \\ --perm 1000 \\ --out Results/Height.perm Figure 1.3: Barplot of Height using 1000 permutations Note 10000 permutations typically provide empirical P-values with high accuracy to the second decimal place (eg. 0.05), but smaller empirical P-values should be considered approximate. Questions \u2753 What is the smallest possible empirical P-value when 10000 permutation are performed? \u2753 Is the height PRS significantly associated with height after accounting for the over-fitting implicit in identifying the best-fit PRS? How about CAD? Out of Sample Validation The best way to avoid having results that are over-fit is to perform validation on an independent validation data set. We can perform validation of the previous height + covariate analysis with PRSice, using the independent VAL target sample as validation data and the \"best\" P-value threshold predicted in the VAL samples: Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/VAL \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/VAL.height \\ --binary-target F \\ --no-full \\ --bar-levels 0.0680001 \\ --fastscore \\ --cov Target_Data/VAL.covariate \\ --cov-col Sex \\ --out Results/Height.val Figure 1.4: Barplot of Height validation dataset Questions \u2753 Why do we use --bar-levels 0.0680001 --no-full and --fastscore in this script? \u2753 How does the PRS R2 and P -value for the validation data set compare to the analysis on the TAR target data? Is this what you would expect? Why? Exercise 3: Distribution of PRS Many PRS study publications include quantile plots that show an exponential increase in phenotypic value or / Odd Ratios (OR) among the top quantiles (e.g. an S-shaped quantile plot, e.g. Figure 1.6). Figure 1.5: An example of density plot for PRS Figure 1.6: An example of a S-shaped quantile plot This might lead us to believe that individuals with PRS values in the top quantiles have a distinctly di\ufb00erent genetic aetiology compared to the rest of the sample, or that there is epistasis/interactions causing there substantially higher risk. However, when we plot a normally distributed variable (e.g. a PRS) as quantiles on the X-axis then we expect to observe this exponential pattern even when the X variable only has a linear e\ufb00ect on the Y variable. This is because the top (and bottom) quantiles are further away from each other on the absolute scale of the variable and so the di\ufb00erences in their e\ufb00ects are larger than between quantiles in the middle of the distribution. To understand this more, we will perform a simple simulation using R: # First, we define some simulation parameters n.sample <- 10000 PRS.r2 <- 0.01 # Then, we simulate PRS that follow a random normal distribution prs <- rnorm (n.sample) # We can then simulate the phenotype using the following script pheno <- prs + rnorm (n.sample,mean =0 , sd = sqrt ( var (prs) * ( 1- PRS.r2) / (PRS.r2))) # We can examine the relationship between the phenotype and prs # using linear regression summary ( lm (pheno ~ prs)) # Which shows that we have the expected PRS R2 # Group the phenotype and PRS into a data.frame info <- data.frame (SampleID =1: n.sample, PRS = prs, Phenotype = pheno) # Then we can generate the quantile plot. # To save time, we will load in the quantile plot script from Software source ( \"./Software/Quantile.R\" ) # Then we can plot the quantile plot using quantile_plot function quantile_plot (info, \"Results/Height\" , 100 ) Figure 1.7: The resulting quantile plot Questions \u2753 What is the shape of the resulting quantile plot? Try plotting the densities of the height or CAD PRS in R * - do they look normally distributed? Why? (*Hint: You can generate a density plot for the PRS in R using plot(density(x)) where x is a vector of the PRS values in the sample). Back to Top","title":"Day2 (PM)"},{"location":"practicals/workshop_practical_paul2/#day-2-afternoon-practical-pathway-prs-analyses","text":"","title":"Day 2 - Afternoon practical: Pathway PRS analyses"},{"location":"practicals/workshop_practical_paul2/#key-learning-outcomes","text":"After completing this practical, you should be able to: Understand the motivation and rationale for calculating gene set PRS. Understand the inputs required for gene set PRS analysis using PRSet. Calculate gene set based PRSs using PRSet. Understand and interpret the outcomes of gene set PRSs and how they differ from genome-wide PRS.","title":"Key Learning Outcomes"},{"location":"practicals/workshop_practical_paul2/#data-structure","text":"You will find all practical materials in the following link . Relevant materials that you should find at the start of the practical are: \ud83d\udcc2: Base_Data GIANT_Height.txt \ud83d\udcc2: Target_Data TAR.fam TAR.bim TAR.bed TAR.height TAR.covariate \ud83d\udcc1: Reference Homo_sapiens.GRCh38.109.gtf.gz Sets.gmt \ud83d\udcc1: Software PRSice.R PRSice_mac/linux","title":"Data Structure"},{"location":"practicals/workshop_practical_paul2/#introduction-to-gene-set-pathway-prs-analysis","text":"Most PRS methods summarize genetic risk to a single number, based on the aggregation of an individual\u2019s genome-wide risk alleles. This approach does not consider the different contributions of the various biological processes that can influence complex diseases and traits. During this session, you will learn how to run a gene set (or pathway) based PRS analyses. The key difference between genome-wide PRS and gene set or pathway-based PRSs analyses is that, instead of aggregating the estimated effects of risk alleles across the entire genome, gene set PRSs aggregate risk alleles across as many gene sets as the user defines (Figure 1). Figure 1: The pathway polygenic risk score approach. Coloured boxes represent genes, lines link genes that are within the same genomic pathway. See full description here . Note In this practical, we will go through some of the input requirements and considerations for the analysis of gene set PRS analysis, and will then calculate some gene set based PRS using PRSet . By aggregating PRS across multiple gene sets (or pathways), these PRS analyses will allow us to determine the genetic contribution made by each biological process in complex traits and diseases. For more information about the rationale and the software that we are going to use, please see the PRSet publication PRSet: Pathway-based polygenic risk score analyses and software . Questions \u2753 Why is it useful to have polygenic scores measured across gene-sets (or pathways) for individuals? \u2753 Isn\u2019t it su\ufb03cient to just obtain a ranking of gene-sets according to GWAS-signal enrichment (using gene set enrichment tools such as MAGMA or partitioned LDSC)?","title":"Introduction to gene set (pathway) PRS analysis"},{"location":"practicals/workshop_practical_paul2/#inputs-required-for-gene-set-prs-analysis","text":"Summary statistics from GWAS, as well as individual level genotype and phenotype data are required to perform gene set PRS analyses. In this session, the following Base and Target data is used. Base data is publicly available. All Target data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only. Additionally, to perform gene set PRS analyses, information about the gene sets for which we want to calculate the PRSs are required. In this tutorial, we will use as input gene-sets from the Molecular Signatures Database (in Reference/ ). Note that PRSet also takes as input BED and SNP files . Data Set Description Download Link Ensembl Human Genome GTF file A file containing the coordinates for genes in the human genome. Used by PRSet to map the SNPs onto genic regions Link to Homo_sapiens.GRCh38.109.gtf.gz MSigDB Gene Sets File containing the gene-set information. Free registration required. Download link after registration","title":"Inputs required for gene-set PRS analysis"},{"location":"practicals/workshop_practical_paul2/#molecular-signatures-database-msigdb-general-transfer-format-file","text":"MSigDB o\ufb00ers an excellent source of gene sets, including the hallmark genes, gene sets of di\ufb00erent biological processes, gene sets of di\ufb00erent oncogenic signatures etc. All gene sets from MSigDB follows the Gene Matrix Transposed file format (GMT), which consists of one line per gene set, each containing at least 3 column of data: Set A Description Gene 1 Gene 2 ... Set B Description Gene 1 Gene 2 ... Tips While you can read the GMT file using Excel, we recommend exploring these files using bash. You should be aware that Excel has a tendency to convert gene names into dates (e.g. SEPT9 to Sep-9) Have a look at the Reference/Sets.gmt file. Questions \u2753 How many gene sets are there in the Reference/Sets.gmt file? \u2753 How many genes does the largest gene set contain? Hint: You can use the command awk '{print $1\"\\t\"NF}' Sets.gmt As GMT format does not contain the chromosomal location for each individual gene, an additional file (General Transfer Format file) is required to provide the chromosomal location such that SNPs can be mapped to genes. The General Transfer Format (GTF) file contains the chromosomal coordinates for each gene. It is a tab separated file and all fields except the last must contain a value. You can read the full format specification here . Two columns in the GTF file that might be of particular interest are: - Column 3: feature , which indicates what feature that line of GTF represents. This allows us to select or ignore features that are of interest. Column 9: attribute , which contains a semicolon-separated list of tag-value pairs (separated by a space), providing additional information about each feature. A key can be repeated multiple times.","title":"Molecular Signatures Database MSigDB + General Transfer Format file"},{"location":"practicals/workshop_practical_paul2/#other-inputs-that-can-be-used-for-gene-set-prs-using-prset","text":"","title":"Other inputs that can be used for gene set PRS using PRSet"},{"location":"practicals/workshop_practical_paul2/#browser-extensible-data-bed","text":"Browser Extensible Data (BED) file (di\ufb00erent to the binary ped file from PLINK), is a file format to define genetic regions. It contains 3 required fields per line (chromosome, start coordinate and end coordinate) together with 9 additional optional field. A special property of BED is that it is a 0-based format, i.e. chromosome starts at 0, as opposed to the usual 1-based format such as the PLINK format. For example, a SNP on chr1:10000 will be represented as: 1 9999 10000 Questions \u2753 How should we represent the coordinate of rs2980300 (chr1:785989) in BED format?","title":"Browser Extensible Data BED"},{"location":"practicals/workshop_practical_paul2/#list-of-snps","text":"Finally, PRSet also allow SNP sets, where the user have flexibility to decide what SNPs are included. The list of SNPs can have two different formats: - SNP list format, a file containing a single column of SNP ID. Name of the set will be the file name or can be provided using --snp-set File:Name - MSigDB format: Each row represent a single SNP set with the first column containing the name of the SNP set.","title":"List of SNPs"},{"location":"practicals/workshop_practical_paul2/#exercise-calculate-gene-set-prs-analysis","text":"To perform the PRSet analysis, we need to provide the GTF file and the GMT file to PRSice. Additionally, we want to specify the number of permutation to calculate competitive P-value calculation using the --set-perm option. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --A1 Allele1 \\ --A2 Allele2 \\ --snp MarkerName \\ --pvalue p \\ --stat b \\ --beta \\ --binary-target F \\ --pheno Target_Data/TAR.height \\ --cov Target_Data/TAR.covariate \\ --out Height.set \\ --gtf Reference/Homo_sapiens.GRCh38.109.gtf.gz \\ --wind-5 5kb \\ --wind-3 1kb \\ --msigdb Reference/Sets.gmt \\ --multi-plot 10 \\ --set-perm 1000 Compare the code to calculate a genome-wide PRS with PRSice (e.g. looking at yesterday's practical) and the code above to run PRSet Questions \u2753 What extra commands are used today to run gene set specific PRSs? What do these additional commands do?","title":"Exercise: Calculate gene set PRS analysis"},{"location":"practicals/workshop_practical_paul2/#results-and-plots-specific-of-gene-set-prs-analyses","text":"","title":"Results and Plots specific of gene set PRS analyses"},{"location":"practicals/workshop_practical_paul2/#competitive-p-value-calculation","text":"When only one region of the genome is used to calculate PRSs (for example a gene set or a pathway PRSs), self-contained and/or competitive tests of association can be performed. The null-hypothesis of self-contained and competitive test statistics is di\ufb00erent: - Null-hypothesis for self-contained test - None of the genes within the gene set are associated with the phenotype. - Null-hypothesis for competitive tests - Genes within the gene set are no more associated with the phenotype than genes outside the gene set. Importantly, in a self-contained test, a bigger gene-set will have a higher likelihood of having a significant P-value from self-contained test, which is not desirable. Therefore, competitive P-values should be calculated to account for gene set size, as shown in Figure 2 . Figure 2 : Examples of significant and non-significant gene sets when running competitive tests of association. Check the .summary results file after running PRSice WITH and WITHOUT including the PRSet specific options Questions \u2753 How does the .summary output file change? What extra information (i.e. extra columns) are incorporated when including PRSet specific commands? \u2753 What are the 3 gene-sets with the smallest self contained P-values? \u2753 What are the 3 gene-sets with the smallest competitive P-values? Imagine that you are running an analysis to find the gene sets most associated with height Questions \u2753 Considering both the competitive P-value results, what gene set do you think is the most interesting and why? In addition to additional information in the output files, running the PRSet options will provide one extra figure with the results for the gene set PRS with the highest R2. Figure 3 : An example of the multi-set plot. Sets are sorted based on their self-contained R2. Base is the genome wide PRS.","title":"Competitive P-value calculation"},{"location":"practicals/workshop_practical_paul2/#other-considerations-when-analysing-and-interpreting-gene-set-prss","text":"","title":"Other considerations when analysing and interpreting gene set PRSs"},{"location":"practicals/workshop_practical_paul2/#clumping-for-each-gene-set-independently","text":"In standard clumping and P-value thresholding methods, clumping is performed to account for linkage disequilibrium between SNPs. However, when performing set based analysis, special care are required to perform clumping. Take the following as an example: Assume that: Light Blue fragments are the intergenic regions Dark Blue fragments are the genic regions Red fragments are the gene set regions SNPs are represented as thunder bolt, with the \"index\" SNP in clumping denoted by the green thunderbolt If we simply perform a genome wide clumping, we might remove all SNPs residing within the gene set of interest, reducing the signal: Therefore, to maximize signal within each gene set, we must perform clumping for each gene sets separately: Check the .summary results file (Do not count the Base, as this result corresponds to the genome-wide PRS) Questions \u2753 Can you plot the relationship between the gene set R2 and the number of SNPs in each gene set? What general trend can be seen?","title":"Clumping for each gene set independently"},{"location":"practicals/workshop_practical_paul2/#p-value-thresholding-in-gene-set-prs-analyses","text":"PRSet default option is to no not perform p-value thresholding. It will simply calculate the set based PRS at P-value threshold of 1. Questions \u2753 Why do you think that the default option of PRSet is P-value threshold of 1? \u2753 In what cases would you like to apply P-value thresholding?","title":"P-value thresholding in gene set PRS analyses"},{"location":"practicals/workshop_practical_paul2/#advanced-polygenic-risk-score-analyses","text":"","title":"Advanced Polygenic Risk Score Analyses"},{"location":"practicals/workshop_practical_paul2/#key-learning-outcomes_1","text":"After completing this practical, you should be able to: Know how to adjust for ascertainment bias in case-control analysis Know how over-fitting a\ufb00ects PRS results and how to handle it Understand distribution of PRS","title":"Key Learning Outcomes"},{"location":"practicals/workshop_practical_paul2/#data-structure_1","text":"Relevant materials that you should find at the start of the practical are: \ud83d\udcc2: Base_Data GIANT_Height.txt Cardio_CAD.txt \ud83d\udcc2: Target_Data TAR.fam TAR.bim TAR.bed TAR.height TAR.cad TAR.covariate VAL.fam VAL.bim VAL.bed VAL.height VAL.covariate \ud83d\udcc1: Software PRSice.R PRSice_mac/linux Quantile.R Warning Note All target phenotype data in this worshop are simulated . They have no specific biological meaning and are for demonstration purposes only.","title":"Data Structure"},{"location":"practicals/workshop_practical_paul2/#exercise-1-estimating-r2-in-case-and-control-studies","text":"Bias in \\(R^2\\) estimation caused by ascertained case/control samples can be adjusted using the equation proposed by Lee et al (2011) , which requires the sample prevalence (case/control ratio) and population prevalence as parameters. This function is implemented in PRSice and the adjustment can be performed by providing the population prevalence to the command --prevalence . Residuals of logistic regression is not well defined, and in PRS analyses, Nagelkerke \\(R^2\\) is usually used to represent the model \\(R^2\\) (this is the default of PRSice). However, this \\(R^2\\) does not account for the di\ufb00erence between sample prevalence (i.e. case-control ratio) and population prevalence, which can lead to bias in the reported \\(R^2\\) (Figure 1.1a). Figure 1.1: Performance of di\ufb00erent \\(R^2\\) when the study contains equal portion of cases and controls (a) Nagelkerke \\(R^2\\) Bias in \\(R^2\\) estimation caused by ascertained case/control samples can be adjusted using the equation proposed by Lee et al. 2011 (Figure 1.1b) , which requires the sample prevalence (case/control ratio) and population prevalence as parameters. This function is implemented in PRSice and the adjustment can be performed by providing the population prevalence to the command --prevalence . Figure 1.1: Performance of di\ufb00erent \\(R^2\\) when the study contains equal portion of cases and controls **(b) Lee adjusted \\(R^2\\) ** Now, account for the ascertainment of the case/control sample by including the population prevalence (let\u2019s assume e.g. 5% here) in the PRSice command to obtain the adjusted (Lee) \\(R^2\\) : Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/Cardio_CAD.txt \\ --target Target_Data/TAR \\ --snp markername \\ --A1 effect_allele \\ --A2 noneffect_allele \\ --chr chr \\ --bp bp_hg19 \\ --stat beta \\ --beta \\ --pvalue p_dgc \\ --pheno Target_Data/TAR.cad \\ --prevalence 0.05 \\ --binary-target T \\ --out Results/CAD.highres.LEER2 The results are written to the \"Results\" directory. Examine the results folder and each file that was generated. For more information about each file type, see here . Note Check the *.summary file in the Results folder where you will find the usual (Nagelkerke) \\(R^2\\) and the adjusted (Lee) \\(R^2\\) . Figure 1.2: Barplot of CAD Lee \\(R^2\\) Tip To speed up the practical, we have generated a smaller gene-set file. If you want the full gene-set file, you can download it from the link above. All target phenotype data in this workshop are simulated. While they reflect the corresponding trait data, they have no specific biological meaning and are for demonstration purposes only. Questions \u2753 Has accounting for the population prevalence a\ufb00ected the \\(R^2\\) ? \u2753 Would you expect a di\ufb00erence between the Nagelkerke \\(R^2\\) and the Lee adjusted \\(R^2\\) if the case/control ratio in the target sample reflects the disease prevalence in the population?","title":"Exercise 1: Estimating \\(R^2\\) in case and control studies"},{"location":"practicals/workshop_practical_paul2/#exercise-2-overfitting-caused-by-model-optimisation","text":"In PRS analyses, the shrinkage or tuning parameter is usually optimized across a wide range of parametric space (e.g. P -value threshold, proportion of causal SNPs). When both optimisation and association testing are performed on the target data, over-fitted results will be obtained. The accuracy and predictive power of over-fitted results are likely to diminish when replicated in an independent data set. A simple solution is to perform permutation to obtain an empirical P -value for the association model, which is implemented in PRSice. Briefly, permutation is performed as follows: Compute the P -value in your original data, denoted as obs.p, at the \"best\" threshold. Then shu\ufb04e the phenotype and obtain the P -value of the \"best\" threshold for this null phenotype, denoted as null.p Repeat 2 \\(N\\) times Calculate the empirical P-value ( \\(P_{emp}\\) ) as: $$ P_{emp} = \\frac{\\sum_{n=1}^NI(P_{null}\\lt P_observed)+1}{N+1} $$ where \\(I(.)\\) is the indicator function. You will have to specify the number of permutation (N) to perform by providing --perm N as a parameter to PRSice. Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/TAR \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/TAR.height \\ --binary-target F \\ --cov Target_Data/TAR.covariate \\ --cov-col Sex \\ --perm 1000 \\ --out Results/Height.perm Figure 1.3: Barplot of Height using 1000 permutations Note 10000 permutations typically provide empirical P-values with high accuracy to the second decimal place (eg. 0.05), but smaller empirical P-values should be considered approximate. Questions \u2753 What is the smallest possible empirical P-value when 10000 permutation are performed? \u2753 Is the height PRS significantly associated with height after accounting for the over-fitting implicit in identifying the best-fit PRS? How about CAD?","title":"Exercise 2: Overfitting caused by model optimisation"},{"location":"practicals/workshop_practical_paul2/#out-of-sample-validation","text":"The best way to avoid having results that are over-fit is to perform validation on an independent validation data set. We can perform validation of the previous height + covariate analysis with PRSice, using the independent VAL target sample as validation data and the \"best\" P-value threshold predicted in the VAL samples: Rscript ./Software/PRSice.R \\ --prsice Software/PRSice_mac \\ --base Base_Data/GIANT_Height.txt \\ --target Target_Data/VAL \\ --snp MarkerName \\ --A1 Allele1 \\ --A2 Allele2 \\ --stat b \\ --beta \\ --pvalue p \\ --pheno Target_Data/VAL.height \\ --binary-target F \\ --no-full \\ --bar-levels 0.0680001 \\ --fastscore \\ --cov Target_Data/VAL.covariate \\ --cov-col Sex \\ --out Results/Height.val Figure 1.4: Barplot of Height validation dataset Questions \u2753 Why do we use --bar-levels 0.0680001 --no-full and --fastscore in this script? \u2753 How does the PRS R2 and P -value for the validation data set compare to the analysis on the TAR target data? Is this what you would expect? Why?","title":"Out of Sample Validation"},{"location":"practicals/workshop_practical_paul2/#exercise-3-distribution-of-prs","text":"Many PRS study publications include quantile plots that show an exponential increase in phenotypic value or / Odd Ratios (OR) among the top quantiles (e.g. an S-shaped quantile plot, e.g. Figure 1.6). Figure 1.5: An example of density plot for PRS Figure 1.6: An example of a S-shaped quantile plot This might lead us to believe that individuals with PRS values in the top quantiles have a distinctly di\ufb00erent genetic aetiology compared to the rest of the sample, or that there is epistasis/interactions causing there substantially higher risk. However, when we plot a normally distributed variable (e.g. a PRS) as quantiles on the X-axis then we expect to observe this exponential pattern even when the X variable only has a linear e\ufb00ect on the Y variable. This is because the top (and bottom) quantiles are further away from each other on the absolute scale of the variable and so the di\ufb00erences in their e\ufb00ects are larger than between quantiles in the middle of the distribution. To understand this more, we will perform a simple simulation using R: # First, we define some simulation parameters n.sample <- 10000 PRS.r2 <- 0.01 # Then, we simulate PRS that follow a random normal distribution prs <- rnorm (n.sample) # We can then simulate the phenotype using the following script pheno <- prs + rnorm (n.sample,mean =0 , sd = sqrt ( var (prs) * ( 1- PRS.r2) / (PRS.r2))) # We can examine the relationship between the phenotype and prs # using linear regression summary ( lm (pheno ~ prs)) # Which shows that we have the expected PRS R2 # Group the phenotype and PRS into a data.frame info <- data.frame (SampleID =1: n.sample, PRS = prs, Phenotype = pheno) # Then we can generate the quantile plot. # To save time, we will load in the quantile plot script from Software source ( \"./Software/Quantile.R\" ) # Then we can plot the quantile plot using quantile_plot function quantile_plot (info, \"Results/Height\" , 100 ) Figure 1.7: The resulting quantile plot Questions \u2753 What is the shape of the resulting quantile plot? Try plotting the densities of the height or CAD PRS in R * - do they look normally distributed? Why? (*Hint: You can generate a density plot for the PRS in R using plot(density(x)) where x is a vector of the PRS values in the sample). Back to Top","title":"Exercise 3: Distribution of PRS"},{"location":"practicals/workshop_practical_tade/","text":"Population Genetics And Ancestry Analysis Table of Contents Key Learning Outcomes Practical Data A Portability Problem Population Genetics Basics Allele frequencies Linkage disequilibrium Dimensional Reduction: PCA Key Learning Outcomes After completing this practical, you should be able to: Run mixed ancestry PRS and understand the PRS Portability Problem. Understand principle component analysis and dimensional reduction. Understand basic population genetics and know how to analyze ancestry groups. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds. Practical Specific Data The data/software required for this practical can be found in the folder day1b . Inside of this folder there are two folders, one for each operating system. Please enter the correct directory, by typing one of the following two commands: cd day1b/mac_version # for users of macOs cd day1b/linux_version # for users of Linux After doing this you should see the following directories: exercise1: Includes Data/Code to run multi-ancestry PRS exercise2: Includes Data/Code for principal component analysis exercise3: Includes Data/Code for population genetics analysis The downloaded code in these directories needs to be made executable on your machine. You can do that now by typing: chmod +x exercise*/code/* Additionally, if you are using macOs where there is extra security, you may need to follow the additional instructions to allow your computer to run software here. Ex 1: Portability Problem The first exercise of this practical takes place in the folder exercise1 . Once inside the folder you should see code and data directories. Looking in the data directory by typing the following command will reveal: ls data/* \ud83c\uddea\ud83c\uddfa: EURO_GWAS.assoc (European Ancestry GWAS Sumstats) \ud83c\uddec\ud83c\udde7: data/ukTarget (Genotype Phenotype data From a population from the UK.) \ud83c\uddef\ud83c\uddf5: data/japanTarget (Genotype Phenotype data from a population from Japan.) To start, run PRSice using the European GWAS and target data from the UK: ./code/PRSice --base data/EURO_GWAS.assoc --target data/ukTarget/ukTarget --binary-target F --pheno data/ukTarget/ukTarget.pheno --out ukRun Verify that this command produce a file called \"ukRun.best\" that contains individual prs-scores in fourth column. This file can compared to the file data/ukTarget/ukTarget.pheno which contains phenotype-values in the third column. \ud83d\udea8 OPTIONAL-CHALLENGE \ud83d\udea8 Using R, Python, or another program, consider calculating the correlation between the PRS and phenotype data in the two files? First read the pseudocode and see if you can follow the strategy. Then give it a try or read the following solutions and make sure that you understand them. Notice the differences in similarities in the programming languages. Hints #1) Step1: Read Both Files in. prs_data = read('ukRun.best') pheno_data = read(\"data/ukTarget/ukTarget.pheno\") #2) Step2: Extract the correct Column from each file . prs_vals = extract_from(prs_data, column 4) pheno_vals = extract_from(pheno_data, column 3) #3) Step3: Calculate the correlation. R2 = calculate_R2_from_data(prs_vals, pheno_vals) Solution (R) R # read-in prs-file prs1 <- read.table(\"ukRun.best\", header = TRUE, sep = \"\", stringsAsFactors = FALSE) prs.data <- prs1[,4] # read-in pheno-file pheno <- read.table(\"data/ukTarget/ukTarget.pheno\", header = TRUE, sep = \"\", stringsAsFactors = FALSE) pheno.data <- pheno[,3] # Create DataFrame combined_data <- data.frame( x = prs.data, y = pheno.data) # Fit a linear model to the data model <- lm(y ~ x, data = combined_data) # Calculate the R-squared value r_squared <- summary(model)$r.squared # return R2 print(r_squared) Solution (Python) python3 # read-in prs-file: with open('ukRun.best') as F: prs_vals = [float(line.split()[-1]) for i,line in enumerate(F.readlines()) if i > 0] # read-in pheno-file: with open('data/ukTarget/ukTarget.pheno') as F: pheno_vals = [float(line.split()[-1]) for i,line in enumerate(F.readlines()) if i > 0] # calculate correlation prs_mean, pheno_mean = sum(prs_vals)/len(prs_vals), sum(pheno_vals)/len(pheno_vals) rTop = sum([(x-prs_mean)*(y-pheno_mean) for x,y in zip(prs_vals, pheno_vals)]) rBottom = (sum([(x-prs_mean)*(x-prs_mean) for x in prs_vals])**0.5) * (sum([(x-pheno_mean)*(x-pheno_mean) for x in pheno_vals])**0.5) # Return R2 R2 = (rTop/rBottom)*(rTop/rBottom) print(R2) After you feel confident about the code, please run the following Rscript in the code directory to calculate the correlation and create a scatterplot using the UK PRS-result: Rscript --vanilla code/plot_prs_results.R data/ukTarget/ukTarget.pheno ukRun.best This will create a scatterplot file called: ukRunScatterplot.pdf . Verify that you can view this pdf. Next type the commands below to reuse the European GWAS data and PRSice with the genotype-phenotype data from Japan. ./code/PRSice --base data/EURO_GWAS.assoc --target data/japanTarget/japanTarget --binary-target F --pheno data/japanTarget/japanTarget.pheno --out japanRun Rscript --vanilla code/plot_prs_results.R data/japanTarget/japanTarget.pheno japanRun.best View the resulting scatterplot and answer the questions below. \u2753QUESTIONS: In the UK-result, what percent of variance in phenotype is explained by prs? In the Japan-result, what percent of variance in phenotype is explained by prs? Besides a difference in variance explained, do you notice any other differences? What is the name of the problem that refers to this drop in performance? What are some causes of the problem? 1. Differences in LD. 2. Differences in allele frequency. 3. Differences in environment. 4. Differences in population-structure. Back to Top 1000 Genomes Data We have observed the PRS-portability problem in practice. Recall from the lecture that the primary drivers of the PRS portability problem are between population difference in allele frequency, linkage disequilibrium and effect sizes. In the next exercises we will use 1000 Genomes (1000G) data to compare allele frequencies and linkage disequilibrium across populations. The 1000G contains individuals from 26 source populations from five super-populations, Europe, East Asia, South Asian, Africa and America. Population structure and population assignment is often accomplished using principal components analysis (PCA). In the final exercise we learn what PCA is and how it can be used to separate population data by recent ancestry. Back to Top Ex 2: Population Genetics This exercise takes place in the folder exercise2 . Once inside the folder you should see code and data directories. Looking in the data directory by typing the following command: ls data/* \ud83c\udf0e: chr1-22.bed/bim/fam (Global Genotype Data) \ud83c\udff7\ufe0f: data/pop_info.pheno (Population specific annotation data) \ud83d\udcab: data/all_phase3.king.psam (Axillary Phase Data) Sample size of each super-population The first thing we would like to find out about this data is the number of individuals in each super-population. Type the following command to query the number of European ancestry individuals in the downloaded dataset: grep -F \"EUR\" data/all_phase3.king.psam | wc -l Next, repeat the same command for East Asian, African, South Asian and American super-populations, by inserting the relevant ancestry codes (EAS, AFR, SAS, AMR). \ud83d\uddd2\ufe0f Make a note of how many individuals there are in each super-population. Number of genetic variants analysed 1000G data contains over 80 million variants genome-wide. The 1000G data we are using in this practical is only a small fraction of these variants. This data gives a reliable approximation for the genomic analyses in this tutorial and importantly, reduces the computation time required to complete the tutorial. The following command counts the number of genetic variants on chromosomes 1-22 used in our analyses wc -l data/chr1-22.bim Number of polymorphic markers in each super-populations The rate at which a genetic variant occurs in a population is known as its allele frequency. Allele frequencies are shaped by evolutionary forces over a long period of time and hence can vary between populations. This has implications for PRS research. The following plink command uses the population information in the file pop_info.pheno to generate allele frequency statistics for each SNP in the five 1000G super-populations: ./code/plink --bfile data/chr1-22 --snps-only --freq --within data/pop_info.pheno Population-stratified allele frequency results are found in the output file plink.frq.strat. Compare the totals against number of SNPs which have minor allele frequencies greater than 0 (and hence are useful for statistical analysis). Do this for all 5 populations (EAS, EUR, SAS, EUR and AFR), using the code below: grep -F \"AFR\" plink.frq.strat > freq_report.afr grep -F \"AMR\" plink.frq.strat > freq_report.amr grep -F \"EUR\" plink.frq.strat > freq_report.eur grep -F \"EAS\" plink.frq.strat > freq_report.eas grep -F \"SAS\" plink.frq.strat > freq_report.sas grep -F \"AFR\" plink.frq.strat | awk '$6 >0' freq_report.afr | wc -l grep -F \"EUR\" plink.frq.strat | awk '$6 >0' freq_report.eur | wc -l grep -F \"EAS\" plink.frq.strat | awk '$6 >0' freq_report.eas | wc -l grep -F \"AMR\" plink.frq.strat | awk '$6 >0' freq_report.amr | wc -l grep -F \"SAS\" plink.frq.strat | awk '$6 >0' freq_report.sas | wc -l Having compared the number of SNPs that show variation in each population, answer the following questions: \u2753QUESTIONS: Which populations have the largest number (density) of SNPs that can be considered polymorphic? What do you think is the significance of the observed population order? Allele frequency variation across the super-populations Here we compare the distribution of allele frequency in the five ancestral populations. To do this we will use the previously-generated output on minor allele frequencies per ancestry group (the file \"plink.frq.strat\"), using R: R-Code: Compare Allele Frequencies library(dplyr) library(ggplot2) freq <-read.table(\"plink.frq.strat\", header =T) plotDat <- freq %>% mutate(AlleleFrequency = cut(MAF, seq(0, 1, 0.25))) %>% group_by(AlleleFrequency, CLST) %>% summarise(FractionOfSNPs = n()/nrow(freq) * 100) ggplot(na.omit(plotDat),aes(AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST)) + geom_line() + scale_y_continuous(limits = c(0, 12)) + ggtitle(\"Distribution of allele frequency across genome\") \u2753QUESTIONS: How are the allele frequencies in AFR distinguishable from the other global reference groups? Linkage disequilibrium variation across populations We will now perform pairwise LD comparisons between genome-wide SNPs to show how the relationship between genomic distance and LD strength varies between populations. We first derive information on pairwise R2 between all SNPs: ./code/plink --bfile data/chr1-22 --keep-cluster-names AFR --within data/pop_info.pheno --r2 --ld-window-r2 0 --ld-window 999999 --ld-window-kb 2500 --threads 30 --out chr1-22.AFR Repeat this step for all five 1000G populations. Output files containing LD info for all pairwise SNPs, have a \u2018.ld\u2019 su\ufb03x Next, create a summary file containing the base-pair distance between each pair and the corresponding R2 value. The following example shows this for AFR and EUR populations only, as just these populations will be used in the plot. cat chr1-22.AFR.ld | sed 1,1d | awk -F \" \" 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS=\"\\t\"}{print abs($5-$2),$7}' | sort -k1,1n > chr1-22.AFR.ld.summary cat chr1-22.EUR.ld | sed 1,1d | awk -F \" \" 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS=\"\\t\"}{print abs($5-$2),$7}' | sort -k1,1n > chr1-22.EUR.ld.summary LD decay versus chromosomal distance The following R commamds use this output to display LD as a function of genomic distance for the African and European populations: R-Code: Visualize LD Behavior # need to add additional functionality to be able to # carry out the necessary data transformation (dplyr) # and manipulation of character strings (stringr ) install.packages(\"dplyr\") install.packages(\"stringr\") install.packages(\"ggplot2\") library(dplyr) library(stringr) library(ggplot2) # Next we will (1) load the previously generated information on pairwise LD, # Categorize distances into intervals of fixed length (100KB), # Compute mean and median r2 within blocks # Obrain mid-points for each distance interval dfr<-read.delim(\"chr1-22.AFR.ld.summary\",sep=\"\",header=F,check.names=F, stringsAsFactors=F) colnames(dfr)<-c(\"dist\",\"rsq\") dfr$distc<-cut(dfr$dist,breaks=seq(from=min(dfr$dist)-1,to=max(dfr$dist)+1,by=100000)) dfr1<-dfr %>% group_by(distc) %>% summarise(mean=mean(rsq),median=median(rsq)) dfr1 <- dfr1 %>% mutate(start=as.integer(str_extract(str_replace_all(distc,\"[\\\\(\\\\)\\\\[\\\\]]\",\"\"),\"^[0-9-e+.]+\")), end=as.integer(str_extract(str_replace_all(distc,\"[\\\\(\\\\)\\\\[\\\\]]\",\"\"),\"[0-9-e+.]+$\")), mid=start+((end-start)/2)) # The preceding code block should be repeated for the file chr1-22._EUR.ld.summary. # When doing so, the output object dfr1 on lines 4 and 5 should be renamed dfr2 to prevent the object df1 being over-written. # Finally, we can plot LD decay for AFR and EUR reference populations in a single graph: ggplot()+ geom_point(data=dfr1,aes(x=start,y=mean),size=0.4,colour=\"grey20\")+ geom_line(data=dfr1,aes(x=start,y=mean),size=0.3,alpha=0.5,colour=\"grey40\")+ labs(x=\"Distance (Megabases)\",y=expression(LD~(r^{2})))+ scale_x_continuous(breaks=c(0,2*10^6,4*10^6,6*10^6,8*10^6),labels=c(\"0\",\"2\",\"4\",\"6\",\"8\"))+ theme_bw() \u2753QUESTIONS: What differences do you observe in terms of LD decay between AFR and EUR genomes? How is this likely to impact the transferability of PRS performance between the two populations? Distribution of LD-block length The next set of scripts will allow us to visualise the distribution of LD block length in the 1000G super-populations. ./code/plink --bfile data/chr1-22 --keep-cluster-names AFR --blocks no-pheno-req no-small-max-span --blocks-max-kb 250 --within data/pop_info.pheno --threads 30 --out AFR The \u201c\u2013block\" flag estimates haplotype blocks using the same block definition implemented by the software Haploview. The default setting for the flag --blocks-max-kb only considers pairs of variants that are within 200 kilobases of each other. The output file from the above command is a .blocks file. Use the same code to generate output for EAS, EUR, SAS and AMR populations (as it is not possible to generate population-specific information using the --within flag). Then, in R: R-Code: Load each of the 5 datasets and set column names to lower case. dfr.afr <- read.delim(\"AFR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.afr) <- tolower(colnames(dfr.afr)) dfr.eur <- read.delim(\"EUR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.eur) <- tolower(colnames(dfr.eur)) dfr.amr <- read.delim(\"AMR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.amr) <- tolower(colnames(dfr.amr)) dfr.sas <- read.delim(\"SAS.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.sas) <- tolower(colnames(dfr.sas)) dfr.eas <- read.delim(\"EAS.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.eas) <- tolower(colnames(dfr.eas)) Then plot the data: plot (density(dfr.afr$kb), main=\"LD block length distribution\", ylab=\"Density\",xlab=\"LD block length (Kb)\" ) lines (density(dfr.eur$kb), col=\"blue\") lines (density(dfr.eas$kb), col=\"red\") lines (density(dfr.amr$kb), col=\"purple\") lines (density(dfr.sas$kb), col=\"green\") legend(\"topright\",c(\"AFR\",\"EAS\",\"EUR\",\"SAS\",\"AMR\"), fill=c(\"black\",\"red\",\"blue\",\"green\",\"purple\")) \u2753QUESTIONS: What are the main features of this plot? How do you interpret them? Back to Top Ex 3: PCA Principle component analysis is a useful technique that allows researchers to visualize high dimensional data in lower space by rotating the axes in such a way that the lower dimensions (or components) maximize the total variance explained. In statistical genetics this involves \"rotating\" million-dimensional data - something that is very hard to visualize! For this reason, we begin with a simpler exercise. For the following three two dimensional shapes, spend some time identifying the principle components or sketching the line across for which variance is maximized. Check your answers below: \u2753QUESTIONS: What line represents the principle component for the first shape? What line represents the principle component for the second shape? What line represents the principle component for the third shape? Below you can view the shapes in principal component space. Now that we understand how PCA works in two dimensions we will consider a higher dimensional example. In the three dimensional space below, see if you can visualize a plane that maximizes the variance across two dimensions: Did you get it right? If so, realize that this is equivalent to what we do in genetics - we find rotate the data through millions of dimensions of space to find the plane that maximizes the variance in two dimensions: To run PCA with real data please enter the exercise3 directory, and type the following command to run PCA on the 1000 Genome data: ./code/plink --bfile data/chr1-22 --indep-pairwise 250 25 0.1 --maf 0.1 --threads 30 --out chr1-22.ldpruned_all_1kgv2 ./code/plink --bfile data/chr1-22 --extract chr1-22.ldpruned_all_1kgv2.prune.in --pca --threads 30 This will generate the principal components that maximize the variance in the data. To plot the result run the following commands from with an R-terminal: R-Code: Generate a PCA Plot require('RColorBrewer') options(scipen=100, digits=3) eigenvec <- read.table('plink.eigenvec', header = F, skip=0, sep = ' ') rownames(eigenvec) <- eigenvec[,2] eigenvec <- eigenvec[,3:ncol(eigenvec)] colnames(eigenvec) <- paste('Principal Component ', c(1:20), sep = '') PED <- read.table(\"data/all_phase3.king.psam\", header = TRUE, skip = 0, sep = '\\t') PED <- PED[which(PED$IID %in% rownames(eigenvec)), ] PED <- PED[match(rownames(eigenvec), PED$IID),] PED$Population <- factor(PED$Population, levels=c(\"ACB\",\"ASW\",\"ESN\",\"GWD\",\"LWK\",\"MSL\",\"YRI\",\"CLM\",\"MXL\",\"PEL\",\"PUR\",\"CDX\",\"CHB\",\"CHS\",\"JPT\",\"KHV\",\"CEU\",\"FIN\",\"GBR\",\"IBS\",\"TSI\",\"BEB\",\"GIH\",\"ITU\",\"PJL\",\"STU\")) col <- colorRampPalette(c(\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\", \"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"black\",\"black\",\"black\",\"black\",\"black\"))(length(unique(PED$Population)))[factor(PED$Population)] project.pca <- eigenvec par(mar = c(5,5,5,5), cex = 2.0,cex.main = 7, cex.axis = 2.75, cex.lab = 2.75, mfrow = c(1,2)) plot(project.pca[,1], project.pca[,2], type = 'n', main = 'A', adj = 0.5, xlab = 'First component', ylab = 'Second component', font = 2, font.lab = 2) points(project.pca[,1], project.pca[,2], col = col, pch = 20, cex = 2.25) legend('bottomright', bty = 'n', cex = 3.0, title = '', c('AFR', 'AMR', 'EAS', 'EUR', 'SAS'), fill = c('yellow', 'forestgreen', 'grey', 'royalblue', 'black')) plot(project.pca[,1], project.pca[,3], type=\"n\", main=\"B\", adj=0.5, xlab=\"First component\", ylab=\"Third component\", font=2, font.lab=2) points(project.pca[,1], project.pca[,3], col=col, pch=20, cex=2.25) \u2753QUESTIONS: What is distinct about the PC projections of the AMR group relative to other populations? Why does this occur? What does it tell us about ancestry of this group?","title":"Day1 (PM)"},{"location":"practicals/workshop_practical_tade/#population-genetics-and-ancestry-analysis","text":"","title":"Population Genetics And Ancestry Analysis"},{"location":"practicals/workshop_practical_tade/#table-of-contents","text":"Key Learning Outcomes Practical Data A Portability Problem Population Genetics Basics Allele frequencies Linkage disequilibrium Dimensional Reduction: PCA","title":"Table of Contents"},{"location":"practicals/workshop_practical_tade/#key-learning-outcomes","text":"After completing this practical, you should be able to: Run mixed ancestry PRS and understand the PRS Portability Problem. Understand principle component analysis and dimensional reduction. Understand basic population genetics and know how to analyze ancestry groups. Understand the challenges and limitations of applying PRS in populations with diverse genetic backgrounds.","title":"Key Learning Outcomes"},{"location":"practicals/workshop_practical_tade/#ex-1-portability-problem","text":"The first exercise of this practical takes place in the folder exercise1 . Once inside the folder you should see code and data directories. Looking in the data directory by typing the following command will reveal: ls data/* \ud83c\uddea\ud83c\uddfa: EURO_GWAS.assoc (European Ancestry GWAS Sumstats) \ud83c\uddec\ud83c\udde7: data/ukTarget (Genotype Phenotype data From a population from the UK.) \ud83c\uddef\ud83c\uddf5: data/japanTarget (Genotype Phenotype data from a population from Japan.) To start, run PRSice using the European GWAS and target data from the UK: ./code/PRSice --base data/EURO_GWAS.assoc --target data/ukTarget/ukTarget --binary-target F --pheno data/ukTarget/ukTarget.pheno --out ukRun Verify that this command produce a file called \"ukRun.best\" that contains individual prs-scores in fourth column. This file can compared to the file data/ukTarget/ukTarget.pheno which contains phenotype-values in the third column. \ud83d\udea8 OPTIONAL-CHALLENGE \ud83d\udea8 Using R, Python, or another program, consider calculating the correlation between the PRS and phenotype data in the two files? First read the pseudocode and see if you can follow the strategy. Then give it a try or read the following solutions and make sure that you understand them. Notice the differences in similarities in the programming languages. Hints #1) Step1: Read Both Files in. prs_data = read('ukRun.best') pheno_data = read(\"data/ukTarget/ukTarget.pheno\") #2) Step2: Extract the correct Column from each file . prs_vals = extract_from(prs_data, column 4) pheno_vals = extract_from(pheno_data, column 3) #3) Step3: Calculate the correlation. R2 = calculate_R2_from_data(prs_vals, pheno_vals) Solution (R) R # read-in prs-file prs1 <- read.table(\"ukRun.best\", header = TRUE, sep = \"\", stringsAsFactors = FALSE) prs.data <- prs1[,4] # read-in pheno-file pheno <- read.table(\"data/ukTarget/ukTarget.pheno\", header = TRUE, sep = \"\", stringsAsFactors = FALSE) pheno.data <- pheno[,3] # Create DataFrame combined_data <- data.frame( x = prs.data, y = pheno.data) # Fit a linear model to the data model <- lm(y ~ x, data = combined_data) # Calculate the R-squared value r_squared <- summary(model)$r.squared # return R2 print(r_squared) Solution (Python) python3 # read-in prs-file: with open('ukRun.best') as F: prs_vals = [float(line.split()[-1]) for i,line in enumerate(F.readlines()) if i > 0] # read-in pheno-file: with open('data/ukTarget/ukTarget.pheno') as F: pheno_vals = [float(line.split()[-1]) for i,line in enumerate(F.readlines()) if i > 0] # calculate correlation prs_mean, pheno_mean = sum(prs_vals)/len(prs_vals), sum(pheno_vals)/len(pheno_vals) rTop = sum([(x-prs_mean)*(y-pheno_mean) for x,y in zip(prs_vals, pheno_vals)]) rBottom = (sum([(x-prs_mean)*(x-prs_mean) for x in prs_vals])**0.5) * (sum([(x-pheno_mean)*(x-pheno_mean) for x in pheno_vals])**0.5) # Return R2 R2 = (rTop/rBottom)*(rTop/rBottom) print(R2) After you feel confident about the code, please run the following Rscript in the code directory to calculate the correlation and create a scatterplot using the UK PRS-result: Rscript --vanilla code/plot_prs_results.R data/ukTarget/ukTarget.pheno ukRun.best This will create a scatterplot file called: ukRunScatterplot.pdf . Verify that you can view this pdf. Next type the commands below to reuse the European GWAS data and PRSice with the genotype-phenotype data from Japan. ./code/PRSice --base data/EURO_GWAS.assoc --target data/japanTarget/japanTarget --binary-target F --pheno data/japanTarget/japanTarget.pheno --out japanRun Rscript --vanilla code/plot_prs_results.R data/japanTarget/japanTarget.pheno japanRun.best View the resulting scatterplot and answer the questions below. \u2753QUESTIONS: In the UK-result, what percent of variance in phenotype is explained by prs? In the Japan-result, what percent of variance in phenotype is explained by prs? Besides a difference in variance explained, do you notice any other differences? What is the name of the problem that refers to this drop in performance? What are some causes of the problem? 1. Differences in LD. 2. Differences in allele frequency. 3. Differences in environment. 4. Differences in population-structure. Back to Top","title":"Ex 1: Portability Problem"},{"location":"practicals/workshop_practical_tade/#1000-genomes-data","text":"We have observed the PRS-portability problem in practice. Recall from the lecture that the primary drivers of the PRS portability problem are between population difference in allele frequency, linkage disequilibrium and effect sizes. In the next exercises we will use 1000 Genomes (1000G) data to compare allele frequencies and linkage disequilibrium across populations. The 1000G contains individuals from 26 source populations from five super-populations, Europe, East Asia, South Asian, Africa and America. Population structure and population assignment is often accomplished using principal components analysis (PCA). In the final exercise we learn what PCA is and how it can be used to separate population data by recent ancestry. Back to Top","title":"1000 Genomes Data"},{"location":"practicals/workshop_practical_tade/#ex-2-population-genetics","text":"This exercise takes place in the folder exercise2 . Once inside the folder you should see code and data directories. Looking in the data directory by typing the following command: ls data/* \ud83c\udf0e: chr1-22.bed/bim/fam (Global Genotype Data) \ud83c\udff7\ufe0f: data/pop_info.pheno (Population specific annotation data) \ud83d\udcab: data/all_phase3.king.psam (Axillary Phase Data)","title":"Ex 2: Population Genetics"},{"location":"practicals/workshop_practical_tade/#sample-size-of-each-super-population","text":"The first thing we would like to find out about this data is the number of individuals in each super-population. Type the following command to query the number of European ancestry individuals in the downloaded dataset: grep -F \"EUR\" data/all_phase3.king.psam | wc -l Next, repeat the same command for East Asian, African, South Asian and American super-populations, by inserting the relevant ancestry codes (EAS, AFR, SAS, AMR). \ud83d\uddd2\ufe0f Make a note of how many individuals there are in each super-population.","title":"Sample size of each super-population"},{"location":"practicals/workshop_practical_tade/#number-of-genetic-variants-analysed","text":"1000G data contains over 80 million variants genome-wide. The 1000G data we are using in this practical is only a small fraction of these variants. This data gives a reliable approximation for the genomic analyses in this tutorial and importantly, reduces the computation time required to complete the tutorial. The following command counts the number of genetic variants on chromosomes 1-22 used in our analyses wc -l data/chr1-22.bim","title":"Number of genetic variants analysed"},{"location":"practicals/workshop_practical_tade/#number-of-polymorphic-markers-in-each-super-populations","text":"The rate at which a genetic variant occurs in a population is known as its allele frequency. Allele frequencies are shaped by evolutionary forces over a long period of time and hence can vary between populations. This has implications for PRS research. The following plink command uses the population information in the file pop_info.pheno to generate allele frequency statistics for each SNP in the five 1000G super-populations: ./code/plink --bfile data/chr1-22 --snps-only --freq --within data/pop_info.pheno Population-stratified allele frequency results are found in the output file plink.frq.strat. Compare the totals against number of SNPs which have minor allele frequencies greater than 0 (and hence are useful for statistical analysis). Do this for all 5 populations (EAS, EUR, SAS, EUR and AFR), using the code below: grep -F \"AFR\" plink.frq.strat > freq_report.afr grep -F \"AMR\" plink.frq.strat > freq_report.amr grep -F \"EUR\" plink.frq.strat > freq_report.eur grep -F \"EAS\" plink.frq.strat > freq_report.eas grep -F \"SAS\" plink.frq.strat > freq_report.sas grep -F \"AFR\" plink.frq.strat | awk '$6 >0' freq_report.afr | wc -l grep -F \"EUR\" plink.frq.strat | awk '$6 >0' freq_report.eur | wc -l grep -F \"EAS\" plink.frq.strat | awk '$6 >0' freq_report.eas | wc -l grep -F \"AMR\" plink.frq.strat | awk '$6 >0' freq_report.amr | wc -l grep -F \"SAS\" plink.frq.strat | awk '$6 >0' freq_report.sas | wc -l Having compared the number of SNPs that show variation in each population, answer the following questions: \u2753QUESTIONS: Which populations have the largest number (density) of SNPs that can be considered polymorphic? What do you think is the significance of the observed population order?","title":"Number of polymorphic markers in each super-populations"},{"location":"practicals/workshop_practical_tade/#allele-frequency-variation-across-the-super-populations","text":"Here we compare the distribution of allele frequency in the five ancestral populations. To do this we will use the previously-generated output on minor allele frequencies per ancestry group (the file \"plink.frq.strat\"), using R: R-Code: Compare Allele Frequencies library(dplyr) library(ggplot2) freq <-read.table(\"plink.frq.strat\", header =T) plotDat <- freq %>% mutate(AlleleFrequency = cut(MAF, seq(0, 1, 0.25))) %>% group_by(AlleleFrequency, CLST) %>% summarise(FractionOfSNPs = n()/nrow(freq) * 100) ggplot(na.omit(plotDat),aes(AlleleFrequency, FractionOfSNPs, group = CLST, col = CLST)) + geom_line() + scale_y_continuous(limits = c(0, 12)) + ggtitle(\"Distribution of allele frequency across genome\") \u2753QUESTIONS: How are the allele frequencies in AFR distinguishable from the other global reference groups?","title":"Allele frequency variation across the super-populations"},{"location":"practicals/workshop_practical_tade/#linkage-disequilibrium-variation-across-populations","text":"We will now perform pairwise LD comparisons between genome-wide SNPs to show how the relationship between genomic distance and LD strength varies between populations. We first derive information on pairwise R2 between all SNPs: ./code/plink --bfile data/chr1-22 --keep-cluster-names AFR --within data/pop_info.pheno --r2 --ld-window-r2 0 --ld-window 999999 --ld-window-kb 2500 --threads 30 --out chr1-22.AFR Repeat this step for all five 1000G populations. Output files containing LD info for all pairwise SNPs, have a \u2018.ld\u2019 su\ufb03x Next, create a summary file containing the base-pair distance between each pair and the corresponding R2 value. The following example shows this for AFR and EUR populations only, as just these populations will be used in the plot. cat chr1-22.AFR.ld | sed 1,1d | awk -F \" \" 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS=\"\\t\"}{print abs($5-$2),$7}' | sort -k1,1n > chr1-22.AFR.ld.summary cat chr1-22.EUR.ld | sed 1,1d | awk -F \" \" 'function abs(v) {return v < 0 ? -v : v}BEGIN{OFS=\"\\t\"}{print abs($5-$2),$7}' | sort -k1,1n > chr1-22.EUR.ld.summary","title":"Linkage disequilibrium variation across populations"},{"location":"practicals/workshop_practical_tade/#ld-decay-versus-chromosomal-distance","text":"The following R commamds use this output to display LD as a function of genomic distance for the African and European populations: R-Code: Visualize LD Behavior # need to add additional functionality to be able to # carry out the necessary data transformation (dplyr) # and manipulation of character strings (stringr ) install.packages(\"dplyr\") install.packages(\"stringr\") install.packages(\"ggplot2\") library(dplyr) library(stringr) library(ggplot2) # Next we will (1) load the previously generated information on pairwise LD, # Categorize distances into intervals of fixed length (100KB), # Compute mean and median r2 within blocks # Obrain mid-points for each distance interval dfr<-read.delim(\"chr1-22.AFR.ld.summary\",sep=\"\",header=F,check.names=F, stringsAsFactors=F) colnames(dfr)<-c(\"dist\",\"rsq\") dfr$distc<-cut(dfr$dist,breaks=seq(from=min(dfr$dist)-1,to=max(dfr$dist)+1,by=100000)) dfr1<-dfr %>% group_by(distc) %>% summarise(mean=mean(rsq),median=median(rsq)) dfr1 <- dfr1 %>% mutate(start=as.integer(str_extract(str_replace_all(distc,\"[\\\\(\\\\)\\\\[\\\\]]\",\"\"),\"^[0-9-e+.]+\")), end=as.integer(str_extract(str_replace_all(distc,\"[\\\\(\\\\)\\\\[\\\\]]\",\"\"),\"[0-9-e+.]+$\")), mid=start+((end-start)/2)) # The preceding code block should be repeated for the file chr1-22._EUR.ld.summary. # When doing so, the output object dfr1 on lines 4 and 5 should be renamed dfr2 to prevent the object df1 being over-written. # Finally, we can plot LD decay for AFR and EUR reference populations in a single graph: ggplot()+ geom_point(data=dfr1,aes(x=start,y=mean),size=0.4,colour=\"grey20\")+ geom_line(data=dfr1,aes(x=start,y=mean),size=0.3,alpha=0.5,colour=\"grey40\")+ labs(x=\"Distance (Megabases)\",y=expression(LD~(r^{2})))+ scale_x_continuous(breaks=c(0,2*10^6,4*10^6,6*10^6,8*10^6),labels=c(\"0\",\"2\",\"4\",\"6\",\"8\"))+ theme_bw() \u2753QUESTIONS: What differences do you observe in terms of LD decay between AFR and EUR genomes? How is this likely to impact the transferability of PRS performance between the two populations?","title":"LD decay versus chromosomal distance"},{"location":"practicals/workshop_practical_tade/#distribution-of-ld-block-length","text":"The next set of scripts will allow us to visualise the distribution of LD block length in the 1000G super-populations. ./code/plink --bfile data/chr1-22 --keep-cluster-names AFR --blocks no-pheno-req no-small-max-span --blocks-max-kb 250 --within data/pop_info.pheno --threads 30 --out AFR The \u201c\u2013block\" flag estimates haplotype blocks using the same block definition implemented by the software Haploview. The default setting for the flag --blocks-max-kb only considers pairs of variants that are within 200 kilobases of each other. The output file from the above command is a .blocks file. Use the same code to generate output for EAS, EUR, SAS and AMR populations (as it is not possible to generate population-specific information using the --within flag). Then, in R: R-Code: Load each of the 5 datasets and set column names to lower case. dfr.afr <- read.delim(\"AFR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.afr) <- tolower(colnames(dfr.afr)) dfr.eur <- read.delim(\"EUR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.eur) <- tolower(colnames(dfr.eur)) dfr.amr <- read.delim(\"AMR.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.amr) <- tolower(colnames(dfr.amr)) dfr.sas <- read.delim(\"SAS.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.sas) <- tolower(colnames(dfr.sas)) dfr.eas <- read.delim(\"EAS.blocks.det\",sep=\"\",header=T,check.names=F,stringsAsFactors=F) colnames(dfr.eas) <- tolower(colnames(dfr.eas)) Then plot the data: plot (density(dfr.afr$kb), main=\"LD block length distribution\", ylab=\"Density\",xlab=\"LD block length (Kb)\" ) lines (density(dfr.eur$kb), col=\"blue\") lines (density(dfr.eas$kb), col=\"red\") lines (density(dfr.amr$kb), col=\"purple\") lines (density(dfr.sas$kb), col=\"green\") legend(\"topright\",c(\"AFR\",\"EAS\",\"EUR\",\"SAS\",\"AMR\"), fill=c(\"black\",\"red\",\"blue\",\"green\",\"purple\")) \u2753QUESTIONS: What are the main features of this plot? How do you interpret them? Back to Top","title":"Distribution of LD-block length"},{"location":"practicals/workshop_practical_tade/#ex-3-pca","text":"Principle component analysis is a useful technique that allows researchers to visualize high dimensional data in lower space by rotating the axes in such a way that the lower dimensions (or components) maximize the total variance explained. In statistical genetics this involves \"rotating\" million-dimensional data - something that is very hard to visualize! For this reason, we begin with a simpler exercise. For the following three two dimensional shapes, spend some time identifying the principle components or sketching the line across for which variance is maximized. Check your answers below: \u2753QUESTIONS: What line represents the principle component for the first shape? What line represents the principle component for the second shape? What line represents the principle component for the third shape? Below you can view the shapes in principal component space. Now that we understand how PCA works in two dimensions we will consider a higher dimensional example. In the three dimensional space below, see if you can visualize a plane that maximizes the variance across two dimensions: Did you get it right? If so, realize that this is equivalent to what we do in genetics - we find rotate the data through millions of dimensions of space to find the plane that maximizes the variance in two dimensions: To run PCA with real data please enter the exercise3 directory, and type the following command to run PCA on the 1000 Genome data: ./code/plink --bfile data/chr1-22 --indep-pairwise 250 25 0.1 --maf 0.1 --threads 30 --out chr1-22.ldpruned_all_1kgv2 ./code/plink --bfile data/chr1-22 --extract chr1-22.ldpruned_all_1kgv2.prune.in --pca --threads 30 This will generate the principal components that maximize the variance in the data. To plot the result run the following commands from with an R-terminal: R-Code: Generate a PCA Plot require('RColorBrewer') options(scipen=100, digits=3) eigenvec <- read.table('plink.eigenvec', header = F, skip=0, sep = ' ') rownames(eigenvec) <- eigenvec[,2] eigenvec <- eigenvec[,3:ncol(eigenvec)] colnames(eigenvec) <- paste('Principal Component ', c(1:20), sep = '') PED <- read.table(\"data/all_phase3.king.psam\", header = TRUE, skip = 0, sep = '\\t') PED <- PED[which(PED$IID %in% rownames(eigenvec)), ] PED <- PED[match(rownames(eigenvec), PED$IID),] PED$Population <- factor(PED$Population, levels=c(\"ACB\",\"ASW\",\"ESN\",\"GWD\",\"LWK\",\"MSL\",\"YRI\",\"CLM\",\"MXL\",\"PEL\",\"PUR\",\"CDX\",\"CHB\",\"CHS\",\"JPT\",\"KHV\",\"CEU\",\"FIN\",\"GBR\",\"IBS\",\"TSI\",\"BEB\",\"GIH\",\"ITU\",\"PJL\",\"STU\")) col <- colorRampPalette(c(\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"yellow\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"forestgreen\",\"grey\",\"grey\",\"grey\",\"grey\",\"grey\", \"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"royalblue\",\"black\",\"black\",\"black\",\"black\",\"black\"))(length(unique(PED$Population)))[factor(PED$Population)] project.pca <- eigenvec par(mar = c(5,5,5,5), cex = 2.0,cex.main = 7, cex.axis = 2.75, cex.lab = 2.75, mfrow = c(1,2)) plot(project.pca[,1], project.pca[,2], type = 'n', main = 'A', adj = 0.5, xlab = 'First component', ylab = 'Second component', font = 2, font.lab = 2) points(project.pca[,1], project.pca[,2], col = col, pch = 20, cex = 2.25) legend('bottomright', bty = 'n', cex = 3.0, title = '', c('AFR', 'AMR', 'EAS', 'EUR', 'SAS'), fill = c('yellow', 'forestgreen', 'grey', 'royalblue', 'black')) plot(project.pca[,1], project.pca[,3], type=\"n\", main=\"B\", adj=0.5, xlab=\"First component\", ylab=\"Third component\", font=2, font.lab=2) points(project.pca[,1], project.pca[,3], col=col, pch=20, cex=2.25) \u2753QUESTIONS: What is distinct about the PC projections of the AMR group relative to other populations? Why does this occur? What does it tell us about ancestry of this group?","title":"Ex 3: PCA"}]}